{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveGAN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Teammember |                    |\n",
    "|------------|--------------------|\n",
    "| 1.         | Christopher Caldwell |\n",
    "| 2.         | Fabian MÃ¼ller      |\n",
    "| 3.         | An Dang         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read as wavread\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preping the Data\n",
    "\n",
    "Here comes all the code for loading and preparing the Musicdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(fp, fs=None, num_channels=1, normalize=False, fast_wav=False):\n",
    "    \"\"\"Decodes audio file paths into 32-bit floating point vectors.\n",
    "    Args:\n",
    "        fp: Audio file path.\n",
    "        fs: If specified, resamples decoded audio to this rate.\n",
    "        mono: If true, averages channels to mono.\n",
    "        fast_wav: Assume fp is a standard WAV file (PCM 16-bit or float 32-bit).\n",
    "    Returns:\n",
    "        A np.float32 array containing the audio samples at specified sample rate.\n",
    "    \"\"\"\n",
    "    ###DEBUGGING\n",
    "    print(fp)\n",
    "    \n",
    "    if fast_wav:\n",
    "    # Read with scipy wavread (fast).\n",
    "        _fs, _wav = wavread(fp)\n",
    "        \n",
    "        if fs is not None and fs != _fs:\n",
    "            raise NotImplementedError('Scipy cannot resample audio.')\n",
    "        \n",
    "        if _wav.dtype == np.int16:\n",
    "            _wav = _wav.astype(np.float32)\n",
    "            _wav /= 32768.\n",
    "            \n",
    "        elif _wav.dtype == np.float32:\n",
    "            _wav = np.copy(_wav)\n",
    "       \n",
    "        else:\n",
    "              raise NotImplementedError('Scipy cannot process atypical WAV files.')\n",
    "    else:\n",
    "        # Decode with librosa load (slow but supports file formats like mp3).\n",
    "        import librosa\n",
    "        _wav, _fs = librosa.core.load(fp, sr=fs, mono=False)\n",
    "        if _wav.ndim == 2:\n",
    "            _wav = np.swapaxes(_wav, 0, 1)\n",
    "\n",
    "    assert _wav.dtype == np.float32\n",
    "\n",
    "    # At this point, _wav is np.float32 either [nsamps,] or [nsamps, nch].\n",
    "    # We want [nsamps, 1, nch] to mimic 2D shape of spectral feats.\n",
    "    if _wav.ndim == 1:\n",
    "        nsamps = _wav.shape[0]\n",
    "        nch = 1\n",
    "    else:\n",
    "        nsamps, nch = _wav.shape\n",
    "    _wav = np.reshape(_wav, [nsamps, 1, nch])\n",
    " \n",
    "    # Average (mono) or expand (stereo) channels\n",
    "    if nch != num_channels:\n",
    "        if num_channels == 1:\n",
    "            _wav = np.mean(_wav, 2, keepdims=True)\n",
    "        elif nch == 1 and num_channels == 2:\n",
    "            _wav = np.concatenate([_wav, _wav], axis=2)\n",
    "        else:\n",
    "            raise ValueError('Number of audio channels not equal to num specified')\n",
    "\n",
    "    if normalize:\n",
    "        factor = np.max(np.abs(_wav))\n",
    "        if factor > 0:\n",
    "            _wav /= factor\n",
    "\n",
    "    return _wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_extract_and_batch(fps, batch_size, slice_len, decode_fs, \n",
    "                             decode_num_channels, decode_normalize=True,\n",
    "                             decode_fast_wav=False, decode_parallel_calls=1,\n",
    "                             slice_randomize_offset=False, slice_first_only=False,\n",
    "                             slice_overlap_ratio=0, slice_pad_end=False,\n",
    "                             repeat=False,\n",
    "                             shuffle=False,\n",
    "                             shuffle_buffer_size=None,\n",
    "                             prefetch_size=None,\n",
    "                             prefetch_gpu_num=None):\n",
    "    \"\"\"Decodes audio file paths into mini-batches of samples.\n",
    "    Args:\n",
    "        fps: List of audio file paths.\n",
    "        batch_size: Number of items in the batch.\n",
    "        slice_len: Length of the sliceuences in samples or feature timesteps.\n",
    "        decode_fs: (Re-)sample rate for decoded audio files.\n",
    "        decode_num_channels: Number of channels for decoded audio files.\n",
    "        decode_normalize: If false, do not normalize audio waveforms.\n",
    "        decode_fast_wav: If true, uses scipy to decode standard wav files.\n",
    "        decode_parallel_calls: Number of parallel decoding threads.\n",
    "        slice_randomize_offset: If true, randomize starting position for slice.\n",
    "        slice_first_only: If true, only use first slice from each audio file.\n",
    "        slice_overlap_ratio: Ratio of overlap between adjacent slices.\n",
    "        slice_pad_end: If true, allows zero-padded examples from the end of each audio file.\n",
    "        repeat: If true (for training), continuously iterate through the dataset.\n",
    "        shuffle: If true (for training), buffer and shuffle the sliceuences.\n",
    "        shuffle_buffer_size: Number of examples to queue up before grabbing a batch.\n",
    "        prefetch_size: Number of examples to prefetch from the queue.\n",
    "        prefetch_gpu_num: If specified, prefetch examples to GPU.\n",
    "    Returns:\n",
    "        A tuple of np.float32 tensors representing audio waveforms.\n",
    "        audio: [batch_size, slice_len, 1, nch]\n",
    "    \"\"\"\n",
    "  \n",
    "    # Create dataset of filepaths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(fps)\n",
    "\n",
    "    # Shuffle all filepaths every epoch\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(fps))\n",
    "\n",
    "    # Repeat\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    \n",
    "    \n",
    "    def _decode_audio_shaped(fp):\n",
    "        _decode_audio_closure = lambda _fp: decode_audio(_fp, \n",
    "                                                         fs=decode_fs,\n",
    "                                                         num_channels=decode_num_channels,\n",
    "                                                         normalize=decode_normalize,\n",
    "                                                         fast_wav=decode_fast_wav)\n",
    "\n",
    "        audio = tf.py_func(_decode_audio_closure,\n",
    "                           [fp],\n",
    "                           tf.float32,\n",
    "                           stateful=False)\n",
    "        \n",
    "        audio.set_shape([None, 1, decode_num_channels])\n",
    "\n",
    "        return audio\n",
    "\n",
    "    # Decode audio\n",
    "    dataset = dataset.map(_decode_audio_shaped, num_parallel_calls=decode_parallel_calls)\n",
    "\n",
    "    # Parallel\n",
    "    def _slice(audio):\n",
    "        # Calculate hop size\n",
    "        if slice_overlap_ratio < 0:\n",
    "            raise ValueError('Overlap ratio must be greater than 0')\n",
    "        slice_hop = int(round(slice_len * (1. - slice_overlap_ratio)) + 1e-4)\n",
    "        if slice_hop < 1:\n",
    "            raise ValueError('Overlap ratio too high')\n",
    "\n",
    "        # Randomize starting phase:\n",
    "        if slice_randomize_offset:\n",
    "            start = tf.random_uniform([], maxval=slice_len, dtype=tf.int32)\n",
    "            audio = audio[start:]\n",
    "\n",
    "        # Extract sliceuences\n",
    "        audio_slices = tf.contrib.signal.frame(audio, \n",
    "                                               slice_len,\n",
    "                                               slice_hop,\n",
    "                                               pad_end=slice_pad_end,\n",
    "                                               pad_value=0,\n",
    "                                               axis=0)\n",
    "\n",
    "        # Only use first slice if requested\n",
    "        if slice_first_only:\n",
    "            audio_slices = audio_slices[:1]\n",
    "\n",
    "        return audio_slices\n",
    "\n",
    "    def _slice_dataset_wrapper(audio):\n",
    "        audio_slices = _slice(audio)\n",
    "        return tf.data.Dataset.from_tensor_slices(audio_slices)\n",
    "\n",
    "    # Extract parallel sliceuences from both audio and features\n",
    "    dataset = dataset.flat_map(_slice_dataset_wrapper)\n",
    "\n",
    "    # Shuffle examples\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Make batches\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # Prefetch a number of batches\n",
    "    if prefetch_size is not None:\n",
    "        dataset = dataset.prefetch(prefetch_size)\n",
    "        if prefetch_gpu_num is not None and prefetch_gpu_num >= 0:\n",
    "            dataset = dataset.apply(tf.data.experimental.prefetch_to_device('/device:GPU:{}'.format(prefetch_gpu_num)))\n",
    "\n",
    "    # Get tensors\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "  \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neccesary Definitions\n",
    "\n",
    "here come all the neccesary definitions for the Discriminator & Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_transpose(inputs, filters, kernel_width, stride=4, padding='same', upsample='zeros'):\n",
    "    if upsample == 'zeros':\n",
    "        return tf.layers.conv2d_transpose(tf.expand_dims(inputs, axis=1), \n",
    "                                          filters,\n",
    "                                          (1, kernel_width),\n",
    "                                          strides=(1, stride),\n",
    "                                          padding='same')[:, 0]\n",
    "    \n",
    "    # If Upsampling should use nearest neighbor\n",
    "    elif upsample == 'nn':\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        _, w, nch = inputs.get_shape().as_list()\n",
    "\n",
    "        x = inputs\n",
    "\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = tf.image.resize_nearest_neighbor(x, [1, w * stride])\n",
    "        x = x[:, 0]\n",
    "\n",
    "        return tf.layers.conv1d(x, filters, kernel_width, 1, padding='same')\n",
    "  \n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(inputs, alpha=0.2):\n",
    "    return tf.maximum(alpha * inputs, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_phaseshuffle(x, rad, pad_type='reflect'):\n",
    "    b, x_len, nch = x.get_shape().as_list()\n",
    "\n",
    "    phase = tf.random_uniform([], minval=-rad, maxval=rad + 1, dtype=tf.int32)\n",
    "    pad_l = tf.maximum(phase, 0)\n",
    "    pad_r = tf.maximum(-phase, 0)\n",
    "    phase_start = pad_r\n",
    "    x = tf.pad(x, [[0, 0], [pad_l, pad_r], [0, 0]], mode=pad_type)\n",
    "\n",
    "    x = x[:, phase_start:phase_start+x_len]\n",
    "    x.set_shape([b, x_len, nch])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveGAN Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kernel Length of 25 (5x5)\n",
    "\n",
    "def WaveGANDiscriminator(X, reuse_vars=None, kernel_len=25, dim=64, use_batchnorm=False, phaseshuffle_rad=0):\n",
    "  \n",
    "    batch_size = tf.shape(X)[0]\n",
    "    slice_len = int(X.get_shape()[1])\n",
    "\n",
    "    \n",
    "    if use_batchnorm:\n",
    "        batchnorm = lambda x: tf.layers.batch_normalization(x, training=True)\n",
    "    else:\n",
    "        batchnorm = lambda x: x\n",
    "\n",
    "    if phaseshuffle_rad > 0:\n",
    "        phaseshuffle = lambda x: apply_phaseshuffle(x, phaseshuffle_rad)\n",
    "    else:\n",
    "        phaseshuffle = lambda x: x\n",
    "\n",
    "        \n",
    "    # Layer 0\n",
    "    # [16384, 1] -> [4096, 64]\n",
    "    output = X\n",
    "    \n",
    "    with tf.variable_scope('downconv_0'):\n",
    "        output = tf.layers.conv1d(output, dim, kernel_len, 4, padding='SAME')\n",
    "    output = lrelu(output)\n",
    "    output = phaseshuffle(output)\n",
    "\n",
    "    # Layer 1\n",
    "    # [4096, 64] -> [1024, 128]\n",
    "    with tf.variable_scope('downconv_1'):\n",
    "        output = tf.layers.conv1d(output, dim * 2, kernel_len, 4, padding='SAME')\n",
    "        output = batchnorm(output)\n",
    "    output = lrelu(output)\n",
    "    output = phaseshuffle(output)\n",
    "\n",
    "    # Layer 2\n",
    "    # [1024, 128] -> [256, 256]\n",
    "    with tf.variable_scope('downconv_2'):\n",
    "        output = tf.layers.conv1d(output, dim * 4, kernel_len, 4, padding='SAME')\n",
    "        output = batchnorm(output)\n",
    "    output = lrelu(output)\n",
    "    output = phaseshuffle(output)\n",
    "\n",
    "    # Layer 3\n",
    "    # [256, 256] -> [64, 512]\n",
    "    with tf.variable_scope('downconv_3'):\n",
    "        output = tf.layers.conv1d(output, dim * 8, kernel_len, 4, padding='SAME')\n",
    "        output = batchnorm(output)\n",
    "    output = lrelu(output)\n",
    "    output = phaseshuffle(output)\n",
    "\n",
    "    # Layer 4\n",
    "    # [64, 512] -> [16, 1024]\n",
    "    with tf.variable_scope('downconv_4'):\n",
    "        output = tf.layers.conv1d(output, dim * 16, kernel_len, 4, padding='SAME')\n",
    "        output = batchnorm(output)\n",
    "    output = lrelu(output)\n",
    "\n",
    "    # Two seconds -> 16384 samples / second ---> 32768 / 2 seconds\n",
    "    if slice_len == 32768:\n",
    "        # Layer 5\n",
    "        # [32, 1024] -> [16, 2048]\n",
    "        with tf.variable_scope('downconv_5'):\n",
    "            output = tf.layers.conv1d(output, dim * 32, kernel_len, 2, padding='SAME')\n",
    "            output = batchnorm(output)\n",
    "        output = lrelu(output)\n",
    "    \n",
    "    # Four seconds -> 16384 samples / second ---> 65536 / 4 seconds\n",
    "    elif slice_len == 65536:\n",
    "        # Layer 5\n",
    "        # [64, 1024] -> [16, 2048]\n",
    "        with tf.variable_scope('downconv_5'):\n",
    "            output = tf.layers.conv1d(output, dim * 32, kernel_len, 4, padding='SAME')\n",
    "            output = batchnorm(output)\n",
    "        output = lrelu(output)\n",
    "\n",
    "    # Flatten\n",
    "    output = tf.reshape(output, [batch_size, -1])\n",
    "\n",
    "    # Connect to single logit\n",
    "    with tf.variable_scope('output'):\n",
    "        output = tf.layers.dense(output, 1)[:, 0]\n",
    "\n",
    "    # Don't need to aggregate batchnorm update ops like we do for the generator because we only use the discriminator for training\n",
    "\n",
    "    return output\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WaveGANGenerator(z, slice_len=16384, nch=1, kernel_len=25, dim=64, use_batchnorm=False, upsample='zeros', train=False):\n",
    "\n",
    "    assert slice_len in [16384, 32768, 65536]\n",
    "    batch_size = tf.shape(z)[0]\n",
    "\n",
    "    if use_batchnorm:\n",
    "        batchnorm = lambda x: tf.layers.batch_normalization(x, training=train)\n",
    "    else:\n",
    "        batchnorm = lambda x: x\n",
    "\n",
    "    # FC and reshape for convolution\n",
    "    # [100] -> [16, 1024]\n",
    "    dim_mul = 16 if slice_len == 16384 else 32\n",
    "    output = z\n",
    "    with tf.variable_scope('z_project'):\n",
    "        output = tf.layers.dense(output, 4 * 4 * dim * dim_mul)\n",
    "        output = tf.reshape(output, [batch_size, 16, dim * dim_mul])\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "    dim_mul //= 2\n",
    "\n",
    "    # Layer 0\n",
    "    # [16, 1024] -> [64, 512]\n",
    "    with tf.variable_scope('upconv_0'):\n",
    "        output = conv1d_transpose(output, dim * dim_mul, kernel_len, 4, upsample=upsample)\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "    dim_mul //= 2\n",
    "\n",
    "    # Layer 1\n",
    "    # [64, 512] -> [256, 256]\n",
    "    with tf.variable_scope('upconv_1'):\n",
    "        output = conv1d_transpose(output, dim * dim_mul, kernel_len, 4, upsample=upsample)\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "    dim_mul //= 2\n",
    "\n",
    "    # Layer 2\n",
    "    # [256, 256] -> [1024, 128]\n",
    "    with tf.variable_scope('upconv_2'):\n",
    "        output = conv1d_transpose(output, dim * dim_mul, kernel_len, 4, upsample=upsample)\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "    dim_mul //= 2\n",
    "\n",
    "    # Layer 3\n",
    "    # [1024, 128] -> [4096, 64]\n",
    "    with tf.variable_scope('upconv_3'):\n",
    "        output = conv1d_transpose(output, dim * dim_mul, kernel_len, 4, upsample=upsample)\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "\n",
    "    if slice_len == 16384:\n",
    "        # Layer 4\n",
    "        # [4096, 64] -> [16384, nch]\n",
    "        with tf.variable_scope('upconv_4'):\n",
    "            output = conv1d_transpose(output, nch, kernel_len, 4, upsample=upsample)\n",
    "        output = tf.nn.tanh(output)\n",
    "   \n",
    "    elif slice_len == 32768:\n",
    "        # Layer 4\n",
    "        # [4096, 128] -> [16384, 64]\n",
    "        with tf.variable_scope('upconv_4'):\n",
    "            output = conv1d_transpose(output, dim, kernel_len, 4, upsample=upsample)\n",
    "            output = batchnorm(output)\n",
    "        output = tf.nn.relu(output)\n",
    "\n",
    "        # Layer 5\n",
    "        # [16384, 64] -> [32768, nch]\n",
    "        with tf.variable_scope('upconv_5'):\n",
    "            output = conv1d_transpose(output, nch, kernel_len, 2, upsample=upsample)\n",
    "        output = tf.nn.tanh(output)\n",
    "        \n",
    "    elif slice_len == 65536:\n",
    "        # Layer 4\n",
    "        # [4096, 128] -> [16384, 64]\n",
    "        with tf.variable_scope('upconv_4'):\n",
    "            output = conv1d_transpose(output, dim, kernel_len, 4, upsample=upsample)\n",
    "            output = batchnorm(output)\n",
    "        output = tf.nn.relu(output)\n",
    "\n",
    "        # Layer 5\n",
    "        # [16384, 64] -> [65536, nch]\n",
    "        with tf.variable_scope('upconv_5'):\n",
    "            output = conv1d_transpose(output, nch, kernel_len, 4, upsample=upsample)\n",
    "        output = tf.nn.tanh(output)\n",
    "\n",
    "    # Automatically update batchnorm moving averages every time G is used during training\n",
    "    if train and use_batchnorm:\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=tf.get_variable_scope().name)\n",
    "        if slice_len == 16384:\n",
    "            assert len(update_ops) == 10\n",
    "        else:\n",
    "            assert len(update_ops) == 12\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            output = tf.identity(output)\n",
    "\n",
    "    return output\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fps, args):\n",
    "    with tf.name_scope('loader'):\n",
    "        x = decode_extract_and_batch(\n",
    "            fps,\n",
    "            batch_size=args.train_batch_size,\n",
    "            slice_len=args.data_slice_len,\n",
    "            decode_fs=args.data_sample_rate,\n",
    "            decode_num_channels=args.data_num_channels,\n",
    "            decode_fast_wav=args.data_fast_wav,\n",
    "            decode_parallel_calls=4,\n",
    "            slice_randomize_offset=False if args.data_first_slice else True,\n",
    "            slice_first_only=args.data_first_slice,\n",
    "            slice_overlap_ratio=0. if args.data_first_slice else args.data_overlap_ratio,\n",
    "            slice_pad_end=True if args.data_first_slice else args.data_pad_end,\n",
    "            repeat=True,\n",
    "            shuffle=True,\n",
    "            shuffle_buffer_size=4096,\n",
    "            prefetch_size=args.train_batch_size * 4,\n",
    "            prefetch_gpu_num=args.data_prefetch_gpu_num)[:, :, 0]\n",
    "\n",
    "    # Make z vector\n",
    "    z = tf.random_uniform([args.train_batch_size, args.wavegan_latent_dim], -1., 1., dtype=tf.float32)\n",
    "\n",
    "    # Make generator\n",
    "    with tf.variable_scope('G'):\n",
    "        G_z = WaveGANGenerator(z, train=True, **args.wavegan_g_kwargs)\n",
    "        if args.wavegan_genr_pp:\n",
    "            with tf.variable_scope('pp_filt'):\n",
    "                G_z = tf.layers.conv1d(G_z, 1, args.wavegan_genr_pp_len, use_bias=False, padding='same')\n",
    "    G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='G')\n",
    "\n",
    "    # Print G summary\n",
    "    print('-' * 80)\n",
    "    print('Generator vars')\n",
    "    nparams = 0\n",
    "    for v in G_vars:\n",
    "        v_shape = v.get_shape().as_list()\n",
    "        v_n = reduce(lambda x, y: x * y, v_shape)\n",
    "        nparams += v_n\n",
    "        print('{} ({}): {}'.format(v.get_shape().as_list(), v_n, v.name))\n",
    "    print('Total params: {} ({:.2f} MB)'.format(nparams, (float(nparams) * 4) / (1024 * 1024)))\n",
    "\n",
    "    # Summarize\n",
    "    tf.summary.audio('x', x, args.data_sample_rate)\n",
    "    tf.summary.audio('G_z', G_z, args.data_sample_rate)\n",
    "    G_z_rms = tf.sqrt(tf.reduce_mean(tf.square(G_z[:, :, 0]), axis=1))\n",
    "    x_rms = tf.sqrt(tf.reduce_mean(tf.square(x[:, :, 0]), axis=1))\n",
    "    tf.summary.histogram('x_rms_batch', x_rms)\n",
    "    tf.summary.histogram('G_z_rms_batch', G_z_rms)\n",
    "    tf.summary.scalar('x_rms', tf.reduce_mean(x_rms))\n",
    "    tf.summary.scalar('G_z_rms', tf.reduce_mean(G_z_rms))\n",
    "\n",
    "  \n",
    "\n",
    "    # Make real discriminator\n",
    "    with tf.name_scope('D_x'), tf.variable_scope('D'):\n",
    "        D_x = WaveGANDiscriminator(x, **args.wavegan_d_kwargs)\n",
    "    D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='D')\n",
    "\n",
    "    # Print D summary\n",
    "    print('-' * 80)\n",
    "    print('Discriminator vars')\n",
    "    nparams = 0\n",
    "    for v in D_vars:\n",
    "        v_shape = v.get_shape().as_list()\n",
    "        v_n = reduce(lambda x, y: x * y, v_shape)\n",
    "        nparams += v_n\n",
    "        print('{} ({}): {}'.format(v.get_shape().as_list(), v_n, v.name))\n",
    "    print('Total params: {} ({:.2f} MB)'.format(nparams, (float(nparams) * 4) / (1024 * 1024)))\n",
    "    print('-' * 80)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    # Make fake discriminator\n",
    "    with tf.name_scope('D_G_z'), tf.variable_scope('D', reuse=True):\n",
    "        D_G_z = WaveGANDiscriminator(G_z, **args.wavegan_d_kwargs)\n",
    "\n",
    "        \n",
    "        \n",
    "    # Create loss\n",
    "    D_clip_weights = None\n",
    "    if args.wavegan_loss == 'dcgan':\n",
    "        fake = tf.zeros([args.train_batch_size], dtype=tf.float32)\n",
    "        real = tf.ones([args.train_batch_size], dtype=tf.float32)\n",
    "\n",
    "        G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          logits=D_G_z,\n",
    "          labels=real\n",
    "        ))\n",
    "\n",
    "        D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          logits=D_G_z,\n",
    "          labels=fake\n",
    "        ))\n",
    "        D_loss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          logits=D_x,\n",
    "          labels=real\n",
    "        ))\n",
    "\n",
    "        D_loss /= 2.\n",
    "  \n",
    "    elif args.wavegan_loss == 'lsgan':\n",
    "        G_loss = tf.reduce_mean((D_G_z - 1.) ** 2)\n",
    "        D_loss = tf.reduce_mean((D_x - 1.) ** 2)\n",
    "        D_loss += tf.reduce_mean(D_G_z ** 2)\n",
    "        D_loss /= 2.\n",
    "    elif args.wavegan_loss == 'wgan':\n",
    "        G_loss = -tf.reduce_mean(D_G_z)\n",
    "        D_loss = tf.reduce_mean(D_G_z) - tf.reduce_mean(D_x)\n",
    "\n",
    "        with tf.name_scope('D_clip_weights'):\n",
    "            clip_ops = []\n",
    "            for var in D_vars:\n",
    "                clip_bounds = [-.01, .01]\n",
    "                clip_ops.append(tf.assign(var,\n",
    "                                          tf.clip_by_value(var,\n",
    "                                                           clip_bounds[0], \n",
    "                                                           clip_bounds[1])\n",
    "                                         )\n",
    "                               )\n",
    "            D_clip_weights = tf.group(*clip_ops)\n",
    "            \n",
    "            \n",
    "    elif args.wavegan_loss == 'wgan-gp':\n",
    "        G_loss = -tf.reduce_mean(D_G_z)\n",
    "        D_loss = tf.reduce_mean(D_G_z) - tf.reduce_mean(D_x)\n",
    "\n",
    "        alpha = tf.random_uniform(shape=[args.train_batch_size, 1, 1], minval=0., maxval=1.)\n",
    "        differences = G_z - x\n",
    "        interpolates = x + (alpha * differences)\n",
    "        with tf.name_scope('D_interp'), tf.variable_scope('D', reuse=True):\n",
    "            D_interp = WaveGANDiscriminator(interpolates, **args.wavegan_d_kwargs)\n",
    "\n",
    "        LAMBDA = 10\n",
    "        gradients = tf.gradients(D_interp, [interpolates])[0]\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1, 2]))\n",
    "        gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2.)\n",
    "        D_loss += LAMBDA * gradient_penalty\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    tf.summary.scalar('G_loss', G_loss)\n",
    "    tf.summary.scalar('D_loss', D_loss)\n",
    "\n",
    "    # Create (recommended) optimizer\n",
    "    if args.wavegan_loss == 'dcgan':\n",
    "        G_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=2e-4,\n",
    "            beta1=0.5)\n",
    "        D_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=2e-4,\n",
    "            beta1=0.5)\n",
    "    elif args.wavegan_loss == 'lsgan':\n",
    "        G_opt = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=1e-4)\n",
    "        D_opt = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=1e-4)\n",
    "    elif args.wavegan_loss == 'wgan':\n",
    "        G_opt = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=5e-5)\n",
    "        D_opt = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=5e-5)\n",
    "    elif args.wavegan_loss == 'wgan-gp':\n",
    "        G_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=1e-4,\n",
    "            beta1=0.5,\n",
    "            beta2=0.9)\n",
    "        D_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=1e-4,\n",
    "            beta1=0.5,\n",
    "            beta2=0.9)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "  \n",
    "    # Create training ops\n",
    "    G_train_op = G_opt.minimize(G_loss, \n",
    "                                var_list=G_vars,\n",
    "                                global_step=tf.train.get_or_create_global_step())\n",
    "    D_train_op = D_opt.minimize(D_loss, var_list=D_vars)\n",
    "\n",
    "    # Run training\n",
    "    with tf.train.MonitoredTrainingSession(checkpoint_dir=args.train_dir,\n",
    "                                           save_checkpoint_secs=args.train_save_secs,\n",
    "                                           save_summaries_secs=args.train_summary_secs) as sess:\n",
    "        print('-' * 80)\n",
    "        print('Training has started. Please use \\'tensorboard --logdir={}\\' to monitor.'.format(args.train_dir))\n",
    "        while True:\n",
    "            # Train discriminator\n",
    "            for i in xrange(args.wavegan_disc_nupdates):\n",
    "                sess.run(D_train_op)\n",
    "\n",
    "            # Enforce Lipschitz constraint for WGAN\n",
    "            if D_clip_weights is not None:\n",
    "                sess.run(D_clip_weights)\n",
    "\n",
    "            # Train generator\n",
    "            sess.run(G_train_op)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "  Creates and saves a MetaGraphDef for simple inference\n",
    "  Tensors:\n",
    "    'samp_z_n' int32 []: Sample this many latent vectors\n",
    "    'samp_z' float32 [samp_z_n, latent_dim]: Resultant latent vectors\n",
    "    'z:0' float32 [None, latent_dim]: Input latent vectors\n",
    "    'flat_pad:0' int32 []: Number of padding samples to use when flattening batch to a single audio file\n",
    "    'G_z:0' float32 [None, slice_len, 1]: Generated outputs\n",
    "    'G_z_int16:0' int16 [None, slice_len, 1]: Same as above but quantizied to 16-bit PCM samples\n",
    "    'G_z_flat:0' float32 [None, 1]: Outputs flattened into single audio file\n",
    "    'G_z_flat_int16:0' int16 [None, 1]: Same as above but quantized to 16-bit PCM samples\n",
    "  Example usage:\n",
    "    import tensorflow as tf\n",
    "    tf.reset_default_graph()\n",
    "    saver = tf.train.import_meta_graph('infer.meta')\n",
    "    graph = tf.get_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "    saver.restore(sess, 'model.ckpt-10000')\n",
    "    z_n = graph.get_tensor_by_name('samp_z_n:0')\n",
    "    _z = sess.run(graph.get_tensor_by_name('samp_z:0'), {z_n: 10})\n",
    "    z = graph.get_tensor_by_name('G_z:0')\n",
    "    _G_z = sess.run(graph.get_tensor_by_name('G_z:0'), {z: _z})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(args):\n",
    "    infer_dir = os.path.join(args.train_dir, 'infer')\n",
    "    if not os.path.isdir(infer_dir):\n",
    "        os.makedirs(infer_dir)\n",
    "\n",
    "    # Subgraph that generates latent vectors\n",
    "    samp_z_n = tf.placeholder(tf.int32, [], name='samp_z_n')\n",
    "    samp_z = tf.random_uniform([samp_z_n, args.wavegan_latent_dim], -1.0, 1.0, dtype=tf.float32, name='samp_z')\n",
    "\n",
    "    # Input zo\n",
    "    z = tf.placeholder(tf.float32, [None, args.wavegan_latent_dim], name='z')\n",
    "    flat_pad = tf.placeholder(tf.int32, [], name='flat_pad')\n",
    "\n",
    "    # Execute generator\n",
    "    with tf.variable_scope('G'):\n",
    "        G_z = WaveGANGenerator(z, train=False, **args.wavegan_g_kwargs)\n",
    "        if args.wavegan_genr_pp:\n",
    "            with tf.variable_scope('pp_filt'):\n",
    "                G_z = tf.layers.conv1d(G_z, 1, args.wavegan_genr_pp_len, use_bias=False, padding='same')\n",
    "    G_z = tf.identity(G_z, name='G_z')\n",
    "\n",
    "    # Flatten batch\n",
    "    nch = int(G_z.get_shape()[-1])\n",
    "    G_z_padded = tf.pad(G_z, [[0, 0], [0, flat_pad], [0, 0]])\n",
    "    G_z_flat = tf.reshape(G_z_padded, [-1, nch], name='G_z_flat')\n",
    "\n",
    "    # Encode to int16\n",
    "    def float_to_int16(x, name=None):\n",
    "        x_int16 = x * 32767.\n",
    "        x_int16 = tf.clip_by_value(x_int16, -32767., 32767.)\n",
    "        x_int16 = tf.cast(x_int16, tf.int16, name=name)\n",
    "        return x_int16\n",
    "    G_z_int16 = float_to_int16(G_z, name='G_z_int16')\n",
    "    G_z_flat_int16 = float_to_int16(G_z_flat, name='G_z_flat_int16')\n",
    "\n",
    "    # Create saver\n",
    "    G_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='G')\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    saver = tf.train.Saver(G_vars + [global_step])\n",
    "\n",
    "    # Export graph\n",
    "    tf.train.write_graph(tf.get_default_graph(), infer_dir, 'infer.pbtxt')\n",
    "\n",
    "    # Export MetaGraph\n",
    "    infer_metagraph_fp = os.path.join(infer_dir, 'infer.meta')\n",
    "    tf.train.export_meta_graph(\n",
    "        filename=infer_metagraph_fp,\n",
    "        clear_devices=True,\n",
    "        saver_def=saver.as_saver_def())\n",
    "\n",
    "    # Reset graph (in case training afterwards)\n",
    "    tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Generates a preview audio file every time a checkpoint is saved\n",
    "\"\"\"\n",
    "def preview(args):\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.io.wavfile import write as wavwrite\n",
    "    from scipy.signal import freqz\n",
    "\n",
    "    preview_dir = os.path.join(args.train_dir, 'preview')\n",
    "    if not os.path.isdir(preview_dir):\n",
    "        os.makedirs(preview_dir)\n",
    "\n",
    "    # Load graph\n",
    "    infer_metagraph_fp = os.path.join(args.train_dir, 'infer', 'infer.meta')\n",
    "    graph = tf.get_default_graph()\n",
    "    saver = tf.train.import_meta_graph(infer_metagraph_fp)\n",
    "\n",
    "    # Generate or restore z_i and z_o\n",
    "    z_fp = os.path.join(preview_dir, 'z.pkl')\n",
    "    if os.path.exists(z_fp):\n",
    "        with open(z_fp, 'rb') as f:\n",
    "            _zs = pickle.load(f)\n",
    "    else:\n",
    "        # Sample z\n",
    "        samp_feeds = {}\n",
    "        samp_feeds[graph.get_tensor_by_name('samp_z_n:0')] = args.preview_n\n",
    "        samp_fetches = {}\n",
    "        samp_fetches['zs'] = graph.get_tensor_by_name('samp_z:0')\n",
    "        with tf.Session() as sess:\n",
    "            _samp_fetches = sess.run(samp_fetches, samp_feeds)\n",
    "        _zs = _samp_fetches['zs']\n",
    "\n",
    "        # Save z\n",
    "        with open(z_fp, 'wb') as f:\n",
    "            pickle.dump(_zs, f)\n",
    "\n",
    "    # Set up graph for generating preview images\n",
    "    feeds = {}\n",
    "    feeds[graph.get_tensor_by_name('z:0')] = _zs\n",
    "    feeds[graph.get_tensor_by_name('flat_pad:0')] = int(args.data_sample_rate / 2)\n",
    "    fetches = {}\n",
    "    fetches['step'] = tf.train.get_or_create_global_step()\n",
    "    fetches['G_z'] = graph.get_tensor_by_name('G_z:0')\n",
    "    fetches['G_z_flat_int16'] = graph.get_tensor_by_name('G_z_flat_int16:0')\n",
    "    if args.wavegan_genr_pp:\n",
    "        fetches['pp_filter'] = graph.get_tensor_by_name('G/pp_filt/conv1d/kernel:0')[:, 0, 0]\n",
    "\n",
    "    # Summarize\n",
    "    G_z = graph.get_tensor_by_name('G_z_flat:0')\n",
    "    summaries = [\n",
    "      tf.summary.audio('preview', tf.expand_dims(G_z, axis=0), args.data_sample_rate, max_outputs=1)\n",
    "    ]\n",
    "    fetches['summaries'] = tf.summary.merge(summaries)\n",
    "    summary_writer = tf.summary.FileWriter(preview_dir)\n",
    "\n",
    "    # PP Summarize\n",
    "    if args.wavegan_genr_pp:\n",
    "        pp_fp = tf.placeholder(tf.string, [])\n",
    "        pp_bin = tf.read_file(pp_fp)\n",
    "        pp_png = tf.image.decode_png(pp_bin)\n",
    "        pp_summary = tf.summary.image('pp_filt', tf.expand_dims(pp_png, axis=0))\n",
    "\n",
    "    # Loop, waiting for checkpoints\n",
    "    ckpt_fp = None\n",
    "    while True:\n",
    "        latest_ckpt_fp = tf.train.latest_checkpoint(args.train_dir)\n",
    "        if latest_ckpt_fp != ckpt_fp:\n",
    "            print('Preview: {}'.format(latest_ckpt_fp))\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                saver.restore(sess, latest_ckpt_fp)\n",
    "\n",
    "                _fetches = sess.run(fetches, feeds)\n",
    "\n",
    "                _step = _fetches['step']\n",
    "\n",
    "            preview_fp = os.path.join(preview_dir, '{}.wav'.format(str(_step).zfill(8)))\n",
    "            wavwrite(preview_fp, args.data_sample_rate, _fetches['G_z_flat_int16'])\n",
    "\n",
    "            summary_writer.add_summary(_fetches['summaries'], _step)\n",
    "\n",
    "            if args.wavegan_genr_pp:\n",
    "                w, h = freqz(_fetches['pp_filter'])\n",
    "\n",
    "                fig = plt.figure()\n",
    "                plt.title('Digital filter frequncy response')\n",
    "                ax1 = fig.add_subplot(111)\n",
    "\n",
    "                plt.plot(w, 20 * np.log10(abs(h)), 'b')\n",
    "                plt.ylabel('Amplitude [dB]', color='b')\n",
    "                plt.xlabel('Frequency [rad/sample]')\n",
    "\n",
    "                ax2 = ax1.twinx()\n",
    "                angles = np.unwrap(np.angle(h))\n",
    "                plt.plot(w, angles, 'g')\n",
    "                plt.ylabel('Angle (radians)', color='g')\n",
    "                plt.grid()\n",
    "                plt.axis('tight')\n",
    "\n",
    "                _pp_fp = os.path.join(preview_dir, '{}_ppfilt.png'.format(str(_step).zfill(8)))\n",
    "                plt.savefig(_pp_fp)\n",
    "\n",
    "                with tf.Session() as sess:\n",
    "                    _summary = sess.run(pp_summary, {pp_fp: _pp_fp})\n",
    "                    summary_writer.add_summary(_summary, _step)\n",
    "                \n",
    "            print('Done')\n",
    "\n",
    "            ckpt_fp = latest_ckpt_fp\n",
    "\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Computes inception score every time a checkpoint is saved\n",
    "\"\"\"\n",
    "def incept(args):\n",
    "    incept_dir = os.path.join(args.train_dir, 'incept')\n",
    "    if not os.path.isdir(incept_dir):\n",
    "        os.makedirs(incept_dir)\n",
    "\n",
    "    # Load GAN graph\n",
    "    gan_graph = tf.Graph()\n",
    "    with gan_graph.as_default():\n",
    "        infer_metagraph_fp = os.path.join(args.train_dir, 'infer', 'infer.meta')\n",
    "        gan_saver = tf.train.import_meta_graph(infer_metagraph_fp)\n",
    "        score_saver = tf.train.Saver(max_to_keep=1)\n",
    "    gan_z = gan_graph.get_tensor_by_name('z:0')\n",
    "    gan_G_z = gan_graph.get_tensor_by_name('G_z:0')[:, :, 0]\n",
    "    gan_step = gan_graph.get_tensor_by_name('global_step:0')\n",
    "\n",
    "    # Load or generate latents\n",
    "    z_fp = os.path.join(incept_dir, 'z.pkl')\n",
    "    if os.path.exists(z_fp):\n",
    "        with open(z_fp, 'rb') as f:\n",
    "            _zs = pickle.load(f)\n",
    "    else:\n",
    "        gan_samp_z_n = gan_graph.get_tensor_by_name('samp_z_n:0')\n",
    "        gan_samp_z = gan_graph.get_tensor_by_name('samp_z:0')\n",
    "        with tf.Session(graph=gan_graph) as sess:\n",
    "            _zs = sess.run(gan_samp_z, {gan_samp_z_n: args.incept_n})\n",
    "        with open(z_fp, 'wb') as f:\n",
    "            pickle.dump(_zs, f)\n",
    "\n",
    "    # Load classifier graph\n",
    "    incept_graph = tf.Graph()\n",
    "    with incept_graph.as_default():\n",
    "        incept_saver = tf.train.import_meta_graph(args.incept_metagraph_fp)\n",
    "    incept_x = incept_graph.get_tensor_by_name('x:0')\n",
    "    incept_preds = incept_graph.get_tensor_by_name('scores:0')\n",
    "    incept_sess = tf.Session(graph=incept_graph)\n",
    "    incept_saver.restore(incept_sess, args.incept_ckpt_fp)\n",
    "\n",
    "    # Create summaries\n",
    "    summary_graph = tf.Graph()\n",
    "    with summary_graph.as_default():\n",
    "        incept_mean = tf.placeholder(tf.float32, [])\n",
    "        incept_std = tf.placeholder(tf.float32, [])\n",
    "        summaries = [\n",
    "            tf.summary.scalar('incept_mean', incept_mean),\n",
    "            tf.summary.scalar('incept_std', incept_std)\n",
    "        ]\n",
    "        summaries = tf.summary.merge(summaries)\n",
    "    summary_writer = tf.summary.FileWriter(incept_dir)\n",
    "\n",
    "    # Loop, waiting for checkpoints\n",
    "    ckpt_fp = None\n",
    "    _best_score = 0.\n",
    "    while True:\n",
    "        latest_ckpt_fp = tf.train.latest_checkpoint(args.train_dir)\n",
    "        if latest_ckpt_fp != ckpt_fp:\n",
    "            print('Incept: {}'.format(latest_ckpt_fp))\n",
    "\n",
    "            sess = tf.Session(graph=gan_graph)\n",
    "\n",
    "            gan_saver.restore(sess, latest_ckpt_fp)\n",
    "\n",
    "            _step = sess.run(gan_step)\n",
    "\n",
    "            _G_zs = []\n",
    "            for i in xrange(0, args.incept_n, 100):\n",
    "                _G_zs.append(sess.run(gan_G_z, {gan_z: _zs[i:i+100]}))\n",
    "            _G_zs = np.concatenate(_G_zs, axis=0)\n",
    "\n",
    "            _preds = []\n",
    "            for i in xrange(0, args.incept_n, 100):\n",
    "                _preds.append(incept_sess.run(incept_preds, {incept_x: _G_zs[i:i+100]}))\n",
    "            _preds = np.concatenate(_preds, axis=0)\n",
    "\n",
    "            # Split into k groups\n",
    "            _incept_scores = []\n",
    "            split_size = args.incept_n // args.incept_k\n",
    "            for i in xrange(args.incept_k):\n",
    "                _split = _preds[i * split_size:(i + 1) * split_size]\n",
    "                _kl = _split * (np.log(_split) - np.log(np.expand_dims(np.mean(_split, 0), 0)))\n",
    "                _kl = np.mean(np.sum(_kl, 1))\n",
    "                _incept_scores.append(np.exp(_kl))\n",
    "\n",
    "            _incept_mean, _incept_std = np.mean(_incept_scores), np.std(_incept_scores)\n",
    "\n",
    "            # Summarize\n",
    "            with tf.Session(graph=summary_graph) as summary_sess:\n",
    "                _summaries = summary_sess.run(summaries, {incept_mean: _incept_mean, incept_std: _incept_std})\n",
    "            summary_writer.add_summary(_summaries, _step)\n",
    "\n",
    "            # Save\n",
    "            if _incept_mean > _best_score:\n",
    "                score_saver.save(sess, os.path.join(incept_dir, 'best_score'), _step)\n",
    "                _best_score = _incept_mean\n",
    "\n",
    "            sess.close()\n",
    "\n",
    "            print('Done')\n",
    "\n",
    "            ckpt_fp = latest_ckpt_fp\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    incept_sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    data_dir=\"./piano/train/16bit\"\n",
    "    train_dir=\"./train\"\n",
    "    data_sample_rate=24000\n",
    "    data_slice_len=16384\n",
    "    data_num_channels=1\n",
    "    data_overlap_ratio=0.\n",
    "    data_first_slice=False\n",
    "    data_pad_end=False\n",
    "    data_normalize=False\n",
    "    data_fast_wav=True\n",
    "    data_prefetch_gpu_num=0\n",
    "    wavegan_latent_dim=100\n",
    "    wavegan_kernel_len=25\n",
    "    wavegan_dim=64\n",
    "    wavegan_batchnorm=False\n",
    "    wavegan_disc_nupdates=5\n",
    "    wavegan_loss='wgan-gp'\n",
    "    wavegan_genr_upsample='zeros'\n",
    "    wavegan_genr_pp=False\n",
    "    wavegan_genr_pp_len=512\n",
    "    wavegan_disc_phaseshuffle=2\n",
    "    train_batch_size=64\n",
    "    train_save_secs=300\n",
    "    train_summary_secs=120\n",
    "    preview_n=32\n",
    "    incept_metagraph_fp='./eval/inception/infer.meta'\n",
    "    incept_ckpt_fp='./eval/inception/best_acc-103005'\n",
    "    incept_n=5000\n",
    "    incept_k=10\n",
    "    \n",
    "    wavegan_g_kwargs = {\n",
    "    'slice_len': data_slice_len,\n",
    "    'nch': data_num_channels,\n",
    "    'kernel_len': wavegan_kernel_len,\n",
    "    'dim': wavegan_dim,\n",
    "    'use_batchnorm': wavegan_batchnorm,\n",
    "    'upsample': wavegan_genr_upsample\n",
    "    }\n",
    "    \n",
    "    wavegan_d_kwargs = {\n",
    "    'kernel_len': wavegan_kernel_len,\n",
    "    'dim': wavegan_dim,\n",
    "    'use_batchnorm': wavegan_batchnorm,\n",
    "    'phaseshuffle_rad': wavegan_disc_phaseshuffle\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./piano/train/16bit\n"
     ]
    }
   ],
   "source": [
    "print (args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = glob.glob(os.path.join(args.data_dir, '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./piano/train/16bit\\\\00_1.wav', './piano/train/16bit\\\\01_1.wav', './piano/train/16bit\\\\02_1.wav', './piano/train/16bit\\\\03_1.wav', './piano/train/16bit\\\\04_1.wav', './piano/train/16bit\\\\05_1.wav', './piano/train/16bit\\\\06_1.wav', './piano/train/16bit\\\\07_1.wav', './piano/train/16bit\\\\08_1.wav', './piano/train/16bit\\\\09_1.wav', './piano/train/16bit\\\\10_1.wav', './piano/train/16bit\\\\11_1.wav', './piano/train/16bit\\\\12_1.wav', './piano/train/16bit\\\\13_1.wav', './piano/train/16bit\\\\14_1.wav']\n"
     ]
    }
   ],
   "source": [
    "print(fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "from functools import reduce\n",
    "import os\n",
    "import time\n",
    "from six.moves import xrange\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 audio files in specified directory\n",
      "WARNING:tensorflow:From <ipython-input-3-93deac89bd53>:58: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-8-ea00d898fb18>:16: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-1e92107c7120>:7: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d_transpose instead.\n",
      "--------------------------------------------------------------------------------\n",
      "Generator vars\n",
      "[100, 16384] (1638400): G/z_project/dense/kernel:0\n",
      "[16384] (16384): G/z_project/dense/bias:0\n",
      "[1, 25, 512, 1024] (13107200): G/upconv_0/conv2d_transpose/kernel:0\n",
      "[512] (512): G/upconv_0/conv2d_transpose/bias:0\n",
      "[1, 25, 256, 512] (3276800): G/upconv_1/conv2d_transpose/kernel:0\n",
      "[256] (256): G/upconv_1/conv2d_transpose/bias:0\n",
      "[1, 25, 128, 256] (819200): G/upconv_2/conv2d_transpose/kernel:0\n",
      "[128] (128): G/upconv_2/conv2d_transpose/bias:0\n",
      "[1, 25, 64, 128] (204800): G/upconv_3/conv2d_transpose/kernel:0\n",
      "[64] (64): G/upconv_3/conv2d_transpose/bias:0\n",
      "[1, 25, 1, 64] (1600): G/upconv_4/conv2d_transpose/kernel:0\n",
      "[1] (1): G/upconv_4/conv2d_transpose/bias:0\n",
      "Total params: 19065345 (72.73 MB)\n",
      "WARNING:tensorflow:From <ipython-input-7-467bbfa726af>:25: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv1d instead.\n",
      "--------------------------------------------------------------------------------\n",
      "Discriminator vars\n",
      "[25, 1, 64] (1600): D/downconv_0/conv1d/kernel:0\n",
      "[64] (64): D/downconv_0/conv1d/bias:0\n",
      "[25, 64, 128] (204800): D/downconv_1/conv1d/kernel:0\n",
      "[128] (128): D/downconv_1/conv1d/bias:0\n",
      "[25, 128, 256] (819200): D/downconv_2/conv1d/kernel:0\n",
      "[256] (256): D/downconv_2/conv1d/bias:0\n",
      "[25, 256, 512] (3276800): D/downconv_3/conv1d/kernel:0\n",
      "[512] (512): D/downconv_3/conv1d/bias:0\n",
      "[25, 512, 1024] (13107200): D/downconv_4/conv1d/kernel:0\n",
      "[1024] (1024): D/downconv_4/conv1d/bias:0\n",
      "[16384, 1] (16384): D/output/dense/kernel:0\n",
      "[1] (1): D/output/dense/bias:0\n",
      "Total params: 17427969 (66.48 MB)\n",
      "--------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:From c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./train\\model.ckpt-0\n",
      "WARNING:tensorflow:From c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./train\\model.ckpt.\n",
      "--------------------------------------------------------------------------------\n",
      "Training has started. Please use 'tensorboard --logdir=./train' to monitor.\n",
      "b'./piano/train/16bit\\\\00_1.wav'b'./piano/train/16bit\\\\02_1.wav'\n",
      "b'./piano/train/16bit\\\\12_1.wav'\n",
      "b'./piano/train/16bit\\\\03_1.wav'\n",
      "\n",
      "b'./piano/train/16bit\\\\10_1.wav'\n",
      "b'./piano/train/16bit\\\\05_1.wav'\n",
      "b'./piano/train/16bit\\\\09_1.wav'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\scipy\\io\\wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'./piano/train/16bit\\\\01_1.wav'\n",
      "b'./piano/train/16bit\\\\13_1.wav'\n",
      "b'./piano/train/16bit\\\\11_1.wav'\n",
      "b'./piano/train/16bit\\\\04_1.wav'\n",
      "b'./piano/train/16bit\\\\06_1.wav'\n",
      "b'./piano/train/16bit\\\\14_1.wav'\n",
      "b'./piano/train/16bit\\\\07_1.wav'\n",
      "b'./piano/train/16bit\\\\08_1.wav'\n",
      "b'./piano/train/16bit\\\\06_1.wav'\n",
      "b'./piano/train/16bit\\\\10_1.wav'\n",
      "b'./piano/train/16bit\\\\13_1.wav'\n",
      "b'./piano/train/16bit\\\\01_1.wav'\n",
      "b'./piano/train/16bit\\\\02_1.wav'\n",
      "b'./piano/train/16bit\\\\04_1.wav'\n",
      "b'./piano/train/16bit\\\\11_1.wav'\n",
      "b'./piano/train/16bit\\\\12_1.wav'\n",
      "b'./piano/train/16bit\\\\09_1.wav'\n",
      "b'./piano/train/16bit\\\\03_1.wav'\n",
      "b'./piano/train/16bit\\\\00_1.wav'\n",
      "b'./piano/train/16bit\\\\05_1.wav'\n",
      "b'./piano/train/16bit\\\\08_1.wav'\n",
      "b'./piano/train/16bit\\\\14_1.wav'\n",
      "b'./piano/train/16bit\\\\07_1.wav'\n",
      "b'./piano/train/16bit\\\\01_1.wav'\n",
      "b'./piano/train/16bit\\\\07_1.wav'\n",
      "b'./piano/train/16bit\\\\06_1.wav'\n",
      "b'./piano/train/16bit\\\\09_1.wav'\n",
      "b'./piano/train/16bit\\\\05_1.wav'\n",
      "b'./piano/train/16bit\\\\03_1.wav'\n",
      "b'./piano/train/16bit\\\\00_1.wav'\n",
      "b'./piano/train/16bit\\\\12_1.wav'\n",
      "b'./piano/train/16bit\\\\13_1.wav'\n",
      "b'./piano/train/16bit\\\\11_1.wav'\n",
      "b'./piano/train/16bit\\\\08_1.wav'\n",
      "b'./piano/train/16bit\\\\10_1.wav'\n",
      "b'./piano/train/16bit\\\\14_1.wav'\n",
      "b'./piano/train/16bit\\\\04_1.wav'\n",
      "b'./piano/train/16bit\\\\02_1.wav'\n",
      "b'./piano/train/16bit\\\\00_1.wav'\n",
      "b'./piano/train/16bit\\\\08_1.wav'\n",
      "b'./piano/train/16bit\\\\03_1.wav'\n",
      "b'./piano/train/16bit\\\\05_1.wav'\n",
      "b'./piano/train/16bit\\\\02_1.wav'\n",
      "b'./piano/train/16bit\\\\12_1.wav'\n",
      "b'./piano/train/16bit\\\\07_1.wav'\n",
      "b'./piano/train/16bit\\\\01_1.wav'\n",
      "b'./piano/train/16bit\\\\06_1.wav'\n",
      "b'./piano/train/16bit\\\\13_1.wav'\n",
      "b'./piano/train/16bit\\\\09_1.wav'\n",
      "b'./piano/train/16bit\\\\10_1.wav'\n",
      "b'./piano/train/16bit\\\\11_1.wav'\n",
      "b'./piano/train/16bit\\\\04_1.wav'\n",
      "b'./piano/train/16bit\\\\14_1.wav'\n",
      "b'./piano/train/16bit\\\\11_1.wav'\n",
      "b'./piano/train/16bit\\\\13_1.wav'\n",
      "b'./piano/train/16bit\\\\01_1.wav'\n",
      "b'./piano/train/16bit\\\\08_1.wav'\n",
      "b'./piano/train/16bit\\\\12_1.wav'\n",
      "b'./piano/train/16bit\\\\03_1.wav'\n",
      "b'./piano/train/16bit\\\\14_1.wav'\n",
      "b'./piano/train/16bit\\\\07_1.wav'\n",
      "b'./piano/train/16bit\\\\06_1.wav'\n",
      "b'./piano/train/16bit\\\\04_1.wav'\n",
      "b'./piano/train/16bit\\\\09_1.wav'\n",
      "b'./piano/train/16bit\\\\05_1.wav'\n",
      "b'./piano/train/16bit\\\\00_1.wav'\n",
      "b'./piano/train/16bit\\\\02_1.wav'\n",
      "b'./piano/train/16bit\\\\10_1.wav'\n",
      "b'./piano/train/16bit\\\\13_1.wav'\n",
      "b'./piano/train/16bit\\\\14_1.wav'\n",
      "b'./piano/train/16bit\\\\08_1.wav'\n",
      "b'./piano/train/16bit\\\\04_1.wav'\n",
      "b'./piano/train/16bit\\\\12_1.wav'\n",
      "b'./piano/train/16bit\\\\05_1.wav'\n",
      "b'./piano/train/16bit\\\\07_1.wav'\n",
      "b'./piano/train/16bit\\\\03_1.wav'\n",
      "b'./piano/train/16bit\\\\09_1.wav'\n",
      "b'./piano/train/16bit\\\\01_1.wav'\n",
      "b'./piano/train/16bit\\\\06_1.wav'\n",
      "b'./piano/train/16bit\\\\10_1.wav'\n",
      "b'./piano/train/16bit\\\\00_1.wav'\n",
      "b'./piano/train/16bit\\\\02_1.wav'\n",
      "b'./piano/train/16bit\\\\11_1.wav'\n",
      "b'./piano/train/16bit\\\\14_1.wav'\n",
      "b'./piano/train/16bit\\\\04_1.wav'\n",
      "b'./piano/train/16bit\\\\03_1.wav'\n",
      "b'./piano/train/16bit\\\\12_1.wav'\n",
      "b'./piano/train/16bit\\\\09_1.wav'\n",
      "b'./piano/train/16bit\\\\07_1.wav'\n",
      "b'./piano/train/16bit\\\\10_1.wav'\n",
      "b'./piano/train/16bit\\\\08_1.wav'\n",
      "b'./piano/train/16bit\\\\01_1.wav'\n",
      "b'./piano/train/16bit\\\\13_1.wav'\n",
      "b'./piano/train/16bit\\\\11_1.wav'\n",
      "b'./piano/train/16bit\\\\02_1.wav'\n",
      "b'./piano/train/16bit\\\\00_1.wav'\n",
      "b'./piano/train/16bit\\\\06_1.wav'\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "NotImplementedError: Scipy cannot resample audio.\nTraceback (most recent call last):\n\n  File \"c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 207, in __call__\n    ret = func(*args)\n\n  File \"<ipython-input-3-93deac89bd53>\", line 53, in <lambda>\n    fast_wav=decode_fast_wav)\n\n  File \"<ipython-input-2-c79d5f7c8884>\", line 19, in decode_audio\n    raise NotImplementedError('Scipy cannot resample audio.')\n\nNotImplementedError: Scipy cannot resample audio.\n\n\n\t [[{{node PyFuncStateless}}]]\n\t [[{{node IteratorGetNext}}]]\n\t [[{{node RemoteCall}}]]\n\t [[node loader/IteratorGetNext (defined at <ipython-input-3-93deac89bd53>:118) ]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: NotImplementedError: Scipy cannot resample audio.\nTraceback (most recent call last):\n\n  File \"c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 207, in __call__\n    ret = func(*args)\n\n  File \"<ipython-input-3-93deac89bd53>\", line 53, in <lambda>\n    fast_wav=decode_fast_wav)\n\n  File \"<ipython-input-2-c79d5f7c8884>\", line 19, in decode_audio\n    raise NotImplementedError('Scipy cannot resample audio.')\n\nNotImplementedError: Scipy cannot resample audio.\n\n\n\t [[{{node PyFuncStateless}}]]\n\t [[{{node IteratorGetNext}}]]\n\t [[{{node RemoteCall}}]]\n\t [[{{node loader/IteratorGetNext}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-2560c2a0cbf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found {} audio files in specified directory'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#infer(args)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-e8462d9e36b9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(fps, args)\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[1;31m# Train discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwavegan_disc_nupdates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_train_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[1;31m# Enforce Lipschitz constraint for WGAN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    674\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m                           run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1169\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1171\u001b[1;33m                               run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1172\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1268\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1270\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1253\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m       \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1325\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: NotImplementedError: Scipy cannot resample audio.\nTraceback (most recent call last):\n\n  File \"c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 207, in __call__\n    ret = func(*args)\n\n  File \"<ipython-input-3-93deac89bd53>\", line 53, in <lambda>\n    fast_wav=decode_fast_wav)\n\n  File \"<ipython-input-2-c79d5f7c8884>\", line 19, in decode_audio\n    raise NotImplementedError('Scipy cannot resample audio.')\n\nNotImplementedError: Scipy cannot resample audio.\n\n\n\t [[{{node PyFuncStateless}}]]\n\t [[{{node IteratorGetNext}}]]\n\t [[{{node RemoteCall}}]]\n\t [[node loader/IteratorGetNext (defined at <ipython-input-3-93deac89bd53>:118) ]]"
     ]
    }
   ],
   "source": [
    "if len(fps) == 0:\n",
    "    raise Exception('Did not find any audio files in specified directory')\n",
    "print('Found {} audio files in specified directory'.format(len(fps)))\n",
    "#infer(args)\n",
    "train(fps, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html\n",
    "SoundDataset\n",
    "\n",
    "https://arxiv.org/pdf/1802.04208.pdf\n",
    "waveGAN Paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
