{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveGAN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Teammember |                    |\n",
    "|------------|--------------------|\n",
    "| 1.         | Christopher Caldwell |\n",
    "| 2.         | Fabian MÃ¼ller      |\n",
    "| 3.         | An Dang         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read as wavread\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preping the Data\n",
    "\n",
    "Here comes all the code for loading and preparing the Musicdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(fp, fs=None, num_channels=1, normalize=False, fast_wav=False):\n",
    "    \"\"\"Decodes audio file paths into 32-bit floating point vectors.\n",
    "    Args:\n",
    "        fp: Audio file path.\n",
    "        fs: If specified, resamples decoded audio to this rate.\n",
    "        mono: If true, averages channels to mono.\n",
    "        fast_wav: Assume fp is a standard WAV file (PCM 16-bit or float 32-bit).\n",
    "    Returns:\n",
    "        A np.float32 array containing the audio samples at specified sample rate.\n",
    "    \"\"\"\n",
    "    if fast_wav:\n",
    "    # Read with scipy wavread (fast).\n",
    "        _fs, _wav = wavread(fp)\n",
    "        \n",
    "        if fs is not None and fs != _fs:\n",
    "            raise NotImplementedError('Scipy cannot resample audio.')\n",
    "        \n",
    "        if _wav.dtype == np.int16:\n",
    "            _wav = _wav.astype(np.float32)\n",
    "            _wav /= 32768.\n",
    "            \n",
    "        elif _wav.dtype == np.float32:\n",
    "            _wav = np.copy(_wav)\n",
    "       \n",
    "        else:\n",
    "              raise NotImplementedError('Scipy cannot process atypical WAV files.')\n",
    "    else:\n",
    "        # Decode with librosa load (slow but supports file formats like mp3).\n",
    "        import librosa\n",
    "        _wav, _fs = librosa.core.load(fp, sr=fs, mono=False)\n",
    "        if _wav.ndim == 2:\n",
    "            _wav = np.swapaxes(_wav, 0, 1)\n",
    "\n",
    "    assert _wav.dtype == np.float32\n",
    "\n",
    "    # At this point, _wav is np.float32 either [nsamps,] or [nsamps, nch].\n",
    "    # We want [nsamps, 1, nch] to mimic 2D shape of spectral feats.\n",
    "    if _wav.ndim == 1:\n",
    "        nsamps = _wav.shape[0]\n",
    "        nch = 1\n",
    "    else:\n",
    "        nsamps, nch = _wav.shape\n",
    "    _wav = np.reshape(_wav, [nsamps, 1, nch])\n",
    " \n",
    "    # Average (mono) or expand (stereo) channels\n",
    "    if nch != num_channels:\n",
    "        if num_channels == 1:\n",
    "            _wav = np.mean(_wav, 2, keepdims=True)\n",
    "        elif nch == 1 and num_channels == 2:\n",
    "            _wav = np.concatenate([_wav, _wav], axis=2)\n",
    "        else:\n",
    "            raise ValueError('Number of audio channels not equal to num specified')\n",
    "\n",
    "    if normalize:\n",
    "        factor = np.max(np.abs(_wav))\n",
    "        if factor > 0:\n",
    "            _wav /= factor\n",
    "\n",
    "    return _wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_extract_and_batch(fps, batch_size, slice_len, decode_fs, \n",
    "                             decode_num_channels, decode_normalize=True,\n",
    "                             decode_fast_wav=False, decode_parallel_calls=1,\n",
    "                             slice_randomize_offset=False, slice_first_only=False,\n",
    "                             slice_overlap_ratio=0, slice_pad_end=False,\n",
    "                             repeat=False,\n",
    "                             shuffle=False,\n",
    "                             shuffle_buffer_size=None,\n",
    "                             prefetch_size=None,\n",
    "                             prefetch_gpu_num=None):\n",
    "    \"\"\"Decodes audio file paths into mini-batches of samples.\n",
    "    Args:\n",
    "        fps: List of audio file paths.\n",
    "        batch_size: Number of items in the batch.\n",
    "        slice_len: Length of the sliceuences in samples or feature timesteps.\n",
    "        decode_fs: (Re-)sample rate for decoded audio files.\n",
    "        decode_num_channels: Number of channels for decoded audio files.\n",
    "        decode_normalize: If false, do not normalize audio waveforms.\n",
    "        decode_fast_wav: If true, uses scipy to decode standard wav files.\n",
    "        decode_parallel_calls: Number of parallel decoding threads.\n",
    "        slice_randomize_offset: If true, randomize starting position for slice.\n",
    "        slice_first_only: If true, only use first slice from each audio file.\n",
    "        slice_overlap_ratio: Ratio of overlap between adjacent slices.\n",
    "        slice_pad_end: If true, allows zero-padded examples from the end of each audio file.\n",
    "        repeat: If true (for training), continuously iterate through the dataset.\n",
    "        shuffle: If true (for training), buffer and shuffle the sliceuences.\n",
    "        shuffle_buffer_size: Number of examples to queue up before grabbing a batch.\n",
    "        prefetch_size: Number of examples to prefetch from the queue.\n",
    "        prefetch_gpu_num: If specified, prefetch examples to GPU.\n",
    "    Returns:\n",
    "        A tuple of np.float32 tensors representing audio waveforms.\n",
    "        audio: [batch_size, slice_len, 1, nch]\n",
    "    \"\"\"\n",
    "  \n",
    "    # Create dataset of filepaths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(fps)\n",
    "\n",
    "    # Shuffle all filepaths every epoch\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(fps))\n",
    "\n",
    "    # Repeat\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    \n",
    "    \n",
    "    def _decode_audio_shaped(fp):\n",
    "        _decode_audio_closure = lambda _fp: decode_audio(_fp, \n",
    "                                                         fs=decode_fs,\n",
    "                                                         num_channels=decode_num_channels,\n",
    "                                                         normalize=decode_normalize,\n",
    "                                                         fast_wav=decode_fast_wav)\n",
    "\n",
    "        audio = tf.py_func(_decode_audio_closure,\n",
    "                           [fp],\n",
    "                           tf.float32,\n",
    "                           stateful=False)\n",
    "        \n",
    "        audio.set_shape([None, 1, decode_num_channels])\n",
    "\n",
    "        return audio\n",
    "\n",
    "    # Decode audio\n",
    "    dataset = dataset.map(_decode_audio_shaped, num_parallel_calls=decode_parallel_calls)\n",
    "\n",
    "    # Parallel\n",
    "    def _slice(audio):\n",
    "        # Calculate hop size\n",
    "        if slice_overlap_ratio < 0:\n",
    "            raise ValueError('Overlap ratio must be greater than 0')\n",
    "        slice_hop = int(round(slice_len * (1. - slice_overlap_ratio)) + 1e-4)\n",
    "        if slice_hop < 1:\n",
    "            raise ValueError('Overlap ratio too high')\n",
    "\n",
    "        # Randomize starting phase:\n",
    "        if slice_randomize_offset:\n",
    "            start = tf.random_uniform([], maxval=slice_len, dtype=tf.int32)\n",
    "            audio = audio[start:]\n",
    "\n",
    "        # Extract sliceuences\n",
    "        audio_slices = tf.contrib.signal.frame(audio, \n",
    "                                               slice_len,\n",
    "                                               slice_hop,\n",
    "                                               pad_end=slice_pad_end,\n",
    "                                               pad_value=0,\n",
    "                                               axis=0)\n",
    "\n",
    "        # Only use first slice if requested\n",
    "        if slice_first_only:\n",
    "            audio_slices = audio_slices[:1]\n",
    "\n",
    "        return audio_slices\n",
    "\n",
    "    def _slice_dataset_wrapper(audio):\n",
    "        audio_slices = _slice(audio)\n",
    "        return tf.data.Dataset.from_tensor_slices(audio_slices)\n",
    "\n",
    "    # Extract parallel sliceuences from both audio and features\n",
    "    dataset = dataset.flat_map(_slice_dataset_wrapper)\n",
    "\n",
    "    # Shuffle examples\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Make batches\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # Prefetch a number of batches\n",
    "    if prefetch_size is not None:\n",
    "        dataset = dataset.prefetch(prefetch_size)\n",
    "        if prefetch_gpu_num is not None and prefetch_gpu_num >= 0:\n",
    "            dataset = dataset.apply(tf.data.experimental.prefetch_to_device('/device:GPU:{}'.format(prefetch_gpu_num)))\n",
    "\n",
    "    # Get tensors\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "  \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neccesary Definitions\n",
    "\n",
    "here come all the neccesary definitions for the Discriminator & Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_transpose(inputs, filters, kernel_width, stride=4, padding='same', upsample='zeros'):\n",
    "    if upsample == 'zeros':\n",
    "        return tf.layers.conv2d_transpose(tf.expand_dims(inputs, axis=1), \n",
    "                                          filters,\n",
    "                                          (1, kernel_width),\n",
    "                                          strides=(1, stride),\n",
    "                                          padding='same')[:, 0]\n",
    "    \n",
    "    # If Upsampling should use nearest neighbor\n",
    "    elif upsample == 'nn':\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        _, w, nch = inputs.get_shape().as_list()\n",
    "\n",
    "        x = inputs\n",
    "\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = tf.image.resize_nearest_neighbor(x, [1, w * stride])\n",
    "        x = x[:, 0]\n",
    "\n",
    "        return tf.layers.conv1d(x, filters, kernel_width, 1, padding='same')\n",
    "  \n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(inputs, alpha=0.2):\n",
    "    return tf.maximum(alpha * inputs, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_phaseshuffle(x, rad, pad_type='reflect'):\n",
    "    b, x_len, nch = x.get_shape().as_list()\n",
    "\n",
    "    phase = tf.random_uniform([], minval=-rad, maxval=rad + 1, dtype=tf.int32)\n",
    "    pad_l = tf.maximum(phase, 0)\n",
    "    pad_r = tf.maximum(-phase, 0)\n",
    "    phase_start = pad_r\n",
    "    x = tf.pad(x, [[0, 0], [pad_l, pad_r], [0, 0]], mode=pad_type)\n",
    "\n",
    "    x = x[:, phase_start:phase_start+x_len]\n",
    "    x.set_shape([b, x_len, nch])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveGAN Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kernel Length of 25 (5x5)\n",
    "\n",
    "def discriminator(X, reuse_vars=None, kernel_len=25, dim=64, use_batchnorm=False, phaseshufle_rad=0):\n",
    "  \n",
    "    batch_size = tf.shape(X)[0]\n",
    "    slice_len = int(X.get_shape()[1])\n",
    "\n",
    "    \n",
    "    if use_batchnorm:\n",
    "        batchnorm = lambda x: tf.layers.batch_normalization(x, training=True)\n",
    "    else:\n",
    "        batchnorm = lambda x: x\n",
    "\n",
    "    if phaseshuffle_rad > 0:\n",
    "        phaseshuffle = lambda x: apply_phaseshuffle(x, phaseshuffle_rad)\n",
    "    else:\n",
    "        phaseshuffle = lambda x: x\n",
    "\n",
    "        \n",
    "    # Layer 0\n",
    "    # [16384, 1] -> [4096, 64]\n",
    "    output = x\n",
    "    \n",
    "    with tf.variable_scope('downconv_0'):\n",
    "        output = tf.layers.conv1d(output, dim, kernel_len, 4, padding='SAME')\n",
    "    output = lrelu(output)\n",
    "    output = phaseshuffle(output)\n",
    "\n",
    "    # Layer 1\n",
    "    # [4096, 64] -> [1024, 128]\n",
    "    with tf.variable_scope('downconv_1'):\n",
    "        output = tf.layers.conv1d(output, dim * 2, kernel_len, 4, padding='SAME')\n",
    "        output = batchnorm(output)\n",
    "    output = lrelu(output)\n",
    "    output = phaseshuffle(output)\n",
    "\n",
    "    # Layer 2\n",
    "    # [1024, 128] -> [256, 256]\n",
    "    with tf.variable_scope('downconv_2'):\n",
    "        output = tf.layers.conv1d(output, dim * 4, kernel_len, 4, padding='SAME')\n",
    "        output = batchnorm(output)\n",
    "    output = lrelu(output)\n",
    "    output = phaseshuffle(output)\n",
    "\n",
    "    # Layer 3\n",
    "    # [256, 256] -> [64, 512]\n",
    "    with tf.variable_scope('downconv_3'):\n",
    "        output = tf.layers.conv1d(output, dim * 8, kernel_len, 4, padding='SAME')\n",
    "        output = batchnorm(output)\n",
    "    output = lrelu(output)\n",
    "    output = phaseshuffle(output)\n",
    "\n",
    "    # Layer 4\n",
    "    # [64, 512] -> [16, 1024]\n",
    "    with tf.variable_scope('downconv_4'):\n",
    "        output = tf.layers.conv1d(output, dim * 16, kernel_len, 4, padding='SAME')\n",
    "        output = batchnorm(output)\n",
    "    output = lrelu(output)\n",
    "\n",
    "    # Two seconds -> 16384 samples / second ---> 32768 / 2 seconds\n",
    "    if slice_len == 32768:\n",
    "        # Layer 5\n",
    "        # [32, 1024] -> [16, 2048]\n",
    "        with tf.variable_scope('downconv_5'):\n",
    "            output = tf.layers.conv1d(output, dim * 32, kernel_len, 2, padding='SAME')\n",
    "            output = batchnorm(output)\n",
    "        output = lrelu(output)\n",
    "    \n",
    "    # Four seconds -> 16384 samples / second ---> 65536 / 4 seconds\n",
    "    elif slice_len == 65536:\n",
    "        # Layer 5\n",
    "        # [64, 1024] -> [16, 2048]\n",
    "        with tf.variable_scope('downconv_5'):\n",
    "            output = tf.layers.conv1d(output, dim * 32, kernel_len, 4, padding='SAME')\n",
    "            output = batchnorm(output)\n",
    "        output = lrelu(output)\n",
    "\n",
    "    # Flatten\n",
    "    output = tf.reshape(output, [batch_size, -1])\n",
    "\n",
    "    # Connect to single logit\n",
    "    with tf.variable_scope('output'):\n",
    "        output = tf.layers.dense(output, 1)[:, 0]\n",
    "\n",
    "    # Don't need to aggregate batchnorm update ops like we do for the generator because we only use the discriminator for training\n",
    "\n",
    "    return output\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WaveGANGenerator(z, slice_len=16384, nch=1, kernel_len=25, dim=64, use_batchnorm=False, upsample='zeros', train=False):\n",
    "\n",
    "    assert slice_len in [16384, 32768, 65536]\n",
    "    batch_size = tf.shape(z)[0]\n",
    "\n",
    "    if use_batchnorm:\n",
    "        batchnorm = lambda x: tf.layers.batch_normalization(x, training=train)\n",
    "    else:\n",
    "        batchnorm = lambda x: x\n",
    "\n",
    "    # FC and reshape for convolution\n",
    "    # [100] -> [16, 1024]\n",
    "    dim_mul = 16 if slice_len == 16384 else 32\n",
    "    output = z\n",
    "    with tf.variable_scope('z_project'):\n",
    "        output = tf.layers.dense(output, 4 * 4 * dim * dim_mul)\n",
    "        output = tf.reshape(output, [batch_size, 16, dim * dim_mul])\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "    dim_mul //= 2\n",
    "\n",
    "    # Layer 0\n",
    "    # [16, 1024] -> [64, 512]\n",
    "    with tf.variable_scope('upconv_0'):\n",
    "        output = conv1d_transpose(output, dim * dim_mul, kernel_len, 4, upsample=upsample)\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "    dim_mul //= 2\n",
    "\n",
    "    # Layer 1\n",
    "    # [64, 512] -> [256, 256]\n",
    "    with tf.variable_scope('upconv_1'):\n",
    "        output = conv1d_transpose(output, dim * dim_mul, kernel_len, 4, upsample=upsample)\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "    dim_mul //= 2\n",
    "\n",
    "    # Layer 2\n",
    "    # [256, 256] -> [1024, 128]\n",
    "    with tf.variable_scope('upconv_2'):\n",
    "        output = conv1d_transpose(output, dim * dim_mul, kernel_len, 4, upsample=upsample)\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "    dim_mul //= 2\n",
    "\n",
    "    # Layer 3\n",
    "    # [1024, 128] -> [4096, 64]\n",
    "    with tf.variable_scope('upconv_3'):\n",
    "        output = conv1d_transpose(output, dim * dim_mul, kernel_len, 4, upsample=upsample)\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "\n",
    "    if slice_len == 16384:\n",
    "        # Layer 4\n",
    "        # [4096, 64] -> [16384, nch]\n",
    "        with tf.variable_scope('upconv_4'):\n",
    "            output = conv1d_transpose(output, nch, kernel_len, 4, upsample=upsample)\n",
    "        output = tf.nn.tanh(output)\n",
    "   \n",
    "    elif slice_len == 32768:\n",
    "        # Layer 4\n",
    "        # [4096, 128] -> [16384, 64]\n",
    "        with tf.variable_scope('upconv_4'):\n",
    "            output = conv1d_transpose(output, dim, kernel_len, 4, upsample=upsample)\n",
    "            output = batchnorm(output)\n",
    "        output = tf.nn.relu(output)\n",
    "\n",
    "        # Layer 5\n",
    "        # [16384, 64] -> [32768, nch]\n",
    "        with tf.variable_scope('upconv_5'):\n",
    "            output = conv1d_transpose(output, nch, kernel_len, 2, upsample=upsample)\n",
    "        output = tf.nn.tanh(output)\n",
    "        \n",
    "    elif slice_len == 65536:\n",
    "        # Layer 4\n",
    "        # [4096, 128] -> [16384, 64]\n",
    "        with tf.variable_scope('upconv_4'):\n",
    "            output = conv1d_transpose(output, dim, kernel_len, 4, upsample=upsample)\n",
    "            output = batchnorm(output)\n",
    "        output = tf.nn.relu(output)\n",
    "\n",
    "        # Layer 5\n",
    "        # [16384, 64] -> [65536, nch]\n",
    "        with tf.variable_scope('upconv_5'):\n",
    "            output = conv1d_transpose(output, nch, kernel_len, 4, upsample=upsample)\n",
    "        output = tf.nn.tanh(output)\n",
    "\n",
    "    # Automatically update batchnorm moving averages every time G is used during training\n",
    "    if train and use_batchnorm:\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=tf.get_variable_scope().name)\n",
    "        if slice_len == 16384:\n",
    "            assert len(update_ops) == 10\n",
    "        else:\n",
    "            assert len(update_ops) == 12\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            output = tf.identity(output)\n",
    "\n",
    "    return output\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fps, args):\n",
    "    with tf.name_scope('loader'):\n",
    "        x = decode_extract_and_batch(\n",
    "            fps,\n",
    "            batch_size=args.train_batch_size,\n",
    "            slice_len=args.data_slice_len,\n",
    "            decode_fs=args.data_sample_rate,\n",
    "            decode_num_channels=args.data_num_channels,\n",
    "            decode_fast_wav=args.data_fast_wav,\n",
    "            decode_parallel_calls=4,\n",
    "            slice_randomize_offset=False if args.data_first_slice else True,\n",
    "            slice_first_only=args.data_first_slice,\n",
    "            slice_overlap_ratio=0. if args.data_first_slice else args.data_overlap_ratio,\n",
    "            slice_pad_end=True if args.data_first_slice else args.data_pad_end,\n",
    "            repeat=True,\n",
    "            shuffle=True,\n",
    "            shuffle_buffer_size=4096,\n",
    "            prefetch_size=args.train_batch_size * 4,\n",
    "            prefetch_gpu_num=args.data_prefetch_gpu_num)[:, :, 0]\n",
    "\n",
    "    # Make z vector\n",
    "    z = tf.random_uniform([args.train_batch_size, args.wavegan_latent_dim], -1., 1., dtype=tf.float32)\n",
    "\n",
    "    # Make generator\n",
    "    with tf.variable_scope('G'):\n",
    "        G_z = WaveGANGenerator(z, train=True, **args.wavegan_g_kwargs)\n",
    "        if args.wavegan_genr_pp:\n",
    "            with tf.variable_scope('pp_filt'):\n",
    "                G_z = tf.layers.conv1d(G_z, 1, args.wavegan_genr_pp_len, use_bias=False, padding='same')\n",
    "    G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='G')\n",
    "\n",
    "    # Print G summary\n",
    "    print('-' * 80)\n",
    "    print('Generator vars')\n",
    "    nparams = 0\n",
    "    for v in G_vars:\n",
    "        v_shape = v.get_shape().as_list()\n",
    "        v_n = tf.reduce(lambda x, y: x * y, v_shape)\n",
    "        nparams += v_n\n",
    "        print('{} ({}): {}'.format(v.get_shape().as_list(), v_n, v.name))\n",
    "    print('Total params: {} ({:.2f} MB)'.format(nparams, (float(nparams) * 4) / (1024 * 1024)))\n",
    "\n",
    "    # Summarize\n",
    "    tf.summary.audio('x', x, args.data_sample_rate)\n",
    "    tf.summary.audio('G_z', G_z, args.data_sample_rate)\n",
    "    G_z_rms = tf.sqrt(tf.reduce_mean(tf.square(G_z[:, :, 0]), axis=1))\n",
    "    x_rms = tf.sqrt(tf.reduce_mean(tf.square(x[:, :, 0]), axis=1))\n",
    "    tf.summary.histogram('x_rms_batch', x_rms)\n",
    "    tf.summary.histogram('G_z_rms_batch', G_z_rms)\n",
    "    tf.summary.scalar('x_rms', tf.reduce_mean(x_rms))\n",
    "    tf.summary.scalar('G_z_rms', tf.reduce_mean(G_z_rms))\n",
    "\n",
    "  \n",
    "\n",
    "    # Make real discriminator\n",
    "    with tf.name_scope('D_x'), tf.variable_scope('D'):\n",
    "        D_x = WaveGANDiscriminator(x, **args.wavegan_d_kwargs)\n",
    "    D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='D')\n",
    "\n",
    "    # Print D summary\n",
    "    print('-' * 80)\n",
    "    print('Discriminator vars')\n",
    "    nparams = 0\n",
    "    for v in D_vars:\n",
    "        v_shape = v.get_shape().as_list()\n",
    "        v_n = tf.reduce(lambda x, y: x * y, v_shape)\n",
    "        nparams += v_n\n",
    "        print('{} ({}): {}'.format(v.get_shape().as_list(), v_n, v.name))\n",
    "    print('Total params: {} ({:.2f} MB)'.format(nparams, (float(nparams) * 4) / (1024 * 1024)))\n",
    "    print('-' * 80)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    # Make fake discriminator\n",
    "    with tf.name_scope('D_G_z'), tf.variable_scope('D', reuse=True):\n",
    "        D_G_z = WaveGANDiscriminator(G_z, **args.wavegan_d_kwargs)\n",
    "\n",
    "        \n",
    "        \n",
    "    # Create loss\n",
    "    D_clip_weights = None\n",
    "    if args.wavegan_loss == 'dcgan':\n",
    "        fake = tf.zeros([args.train_batch_size], dtype=tf.float32)\n",
    "        real = tf.ones([args.train_batch_size], dtype=tf.float32)\n",
    "\n",
    "        G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          logits=D_G_z,\n",
    "          labels=real\n",
    "        ))\n",
    "\n",
    "        D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          logits=D_G_z,\n",
    "          labels=fake\n",
    "        ))\n",
    "        D_loss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          logits=D_x,\n",
    "          labels=real\n",
    "        ))\n",
    "\n",
    "        D_loss /= 2.\n",
    "  \n",
    "    elif args.wavegan_loss == 'lsgan':\n",
    "        G_loss = tf.reduce_mean((D_G_z - 1.) ** 2)\n",
    "        D_loss = tf.reduce_mean((D_x - 1.) ** 2)\n",
    "        D_loss += tf.reduce_mean(D_G_z ** 2)\n",
    "        D_loss /= 2.\n",
    "    elif args.wavegan_loss == 'wgan':\n",
    "        G_loss = -tf.reduce_mean(D_G_z)\n",
    "        D_loss = tf.reduce_mean(D_G_z) - tf.reduce_mean(D_x)\n",
    "\n",
    "        with tf.name_scope('D_clip_weights'):\n",
    "            clip_ops = []\n",
    "            for var in D_vars:\n",
    "                clip_bounds = [-.01, .01]\n",
    "                clip_ops.append(tf.assign(var,\n",
    "                                          tf.clip_by_value(var,\n",
    "                                                           clip_bounds[0], \n",
    "                                                           clip_bounds[1])\n",
    "                                         )\n",
    "                               )\n",
    "            D_clip_weights = tf.group(*clip_ops)\n",
    "            \n",
    "            \n",
    "    elif args.wavegan_loss == 'wgan-gp':\n",
    "        G_loss = -tf.reduce_mean(D_G_z)\n",
    "        D_loss = tf.reduce_mean(D_G_z) - tf.reduce_mean(D_x)\n",
    "\n",
    "        alpha = tf.random_uniform(shape=[args.train_batch_size, 1, 1], minval=0., maxval=1.)\n",
    "        differences = G_z - x\n",
    "        interpolates = x + (alpha * differences)\n",
    "        with tf.name_scope('D_interp'), tf.variable_scope('D', reuse=True):\n",
    "            D_interp = WaveGANDiscriminator(interpolates, **args.wavegan_d_kwargs)\n",
    "\n",
    "        LAMBDA = 10\n",
    "        gradients = tf.gradients(D_interp, [interpolates])[0]\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1, 2]))\n",
    "        gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2.)\n",
    "        D_loss += LAMBDA * gradient_penalty\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    tf.summary.scalar('G_loss', G_loss)\n",
    "    tf.summary.scalar('D_loss', D_loss)\n",
    "\n",
    "    # Create (recommended) optimizer\n",
    "    if args.wavegan_loss == 'dcgan':\n",
    "        G_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=2e-4,\n",
    "            beta1=0.5)\n",
    "        D_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=2e-4,\n",
    "            beta1=0.5)\n",
    "    elif args.wavegan_loss == 'lsgan':\n",
    "        G_opt = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=1e-4)\n",
    "        D_opt = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=1e-4)\n",
    "    elif args.wavegan_loss == 'wgan':\n",
    "        G_opt = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=5e-5)\n",
    "        D_opt = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=5e-5)\n",
    "    elif args.wavegan_loss == 'wgan-gp':\n",
    "        G_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=1e-4,\n",
    "            beta1=0.5,\n",
    "            beta2=0.9)\n",
    "        D_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=1e-4,\n",
    "            beta1=0.5,\n",
    "            beta2=0.9)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "  \n",
    "    # Create training ops\n",
    "    G_train_op = G_opt.minimize(G_loss, \n",
    "                                var_list=G_vars,\n",
    "                                global_step=tf.train.get_or_create_global_step())\n",
    "    D_train_op = D_opt.minimize(D_loss, var_list=D_vars)\n",
    "\n",
    "    # Run training\n",
    "    with tf.train.MonitoredTrainingSession(checkpoint_dir=args.train_dir,\n",
    "                                           save_checkpoint_secs=args.train_save_secs,\n",
    "                                           save_summaries_secs=args.train_summary_secs) as sess:\n",
    "        print('-' * 80)\n",
    "        print('Training has started. Please use \\'tensorboard --logdir={}\\' to monitor.'.format(args.train_dir))\n",
    "        while True:\n",
    "            # Train discriminator\n",
    "            for i in xrange(args.wavegan_disc_nupdates):\n",
    "                sess.run(D_train_op)\n",
    "\n",
    "            # Enforce Lipschitz constraint for WGAN\n",
    "            if D_clip_weights is not None:\n",
    "                sess.run(D_clip_weights)\n",
    "\n",
    "            # Train generator\n",
    "            sess.run(G_train_op)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "  Creates and saves a MetaGraphDef for simple inference\n",
    "  Tensors:\n",
    "    'samp_z_n' int32 []: Sample this many latent vectors\n",
    "    'samp_z' float32 [samp_z_n, latent_dim]: Resultant latent vectors\n",
    "    'z:0' float32 [None, latent_dim]: Input latent vectors\n",
    "    'flat_pad:0' int32 []: Number of padding samples to use when flattening batch to a single audio file\n",
    "    'G_z:0' float32 [None, slice_len, 1]: Generated outputs\n",
    "    'G_z_int16:0' int16 [None, slice_len, 1]: Same as above but quantizied to 16-bit PCM samples\n",
    "    'G_z_flat:0' float32 [None, 1]: Outputs flattened into single audio file\n",
    "    'G_z_flat_int16:0' int16 [None, 1]: Same as above but quantized to 16-bit PCM samples\n",
    "  Example usage:\n",
    "    import tensorflow as tf\n",
    "    tf.reset_default_graph()\n",
    "    saver = tf.train.import_meta_graph('infer.meta')\n",
    "    graph = tf.get_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "    saver.restore(sess, 'model.ckpt-10000')\n",
    "    z_n = graph.get_tensor_by_name('samp_z_n:0')\n",
    "    _z = sess.run(graph.get_tensor_by_name('samp_z:0'), {z_n: 10})\n",
    "    z = graph.get_tensor_by_name('G_z:0')\n",
    "    _G_z = sess.run(graph.get_tensor_by_name('G_z:0'), {z: _z})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(args):\n",
    "    infer_dir = os.path.join(args.train_dir, 'infer')\n",
    "    if not os.path.isdir(infer_dir):\n",
    "        os.makedirs(infer_dir)\n",
    "\n",
    "    # Subgraph that generates latent vectors\n",
    "    samp_z_n = tf.placeholder(tf.int32, [], name='samp_z_n')\n",
    "    samp_z = tf.random_uniform([samp_z_n, args.wavegan_latent_dim], -1.0, 1.0, dtype=tf.float32, name='samp_z')\n",
    "\n",
    "    # Input zo\n",
    "    z = tf.placeholder(tf.float32, [None, args.wavegan_latent_dim], name='z')\n",
    "    flat_pad = tf.placeholder(tf.int32, [], name='flat_pad')\n",
    "\n",
    "    # Execute generator\n",
    "    with tf.variable_scope('G'):\n",
    "        G_z = WaveGANGenerator(z, train=False, **args.wavegan_g_kwargs)\n",
    "        if args.wavegan_genr_pp:\n",
    "            with tf.variable_scope('pp_filt'):\n",
    "                G_z = tf.layers.conv1d(G_z, 1, args.wavegan_genr_pp_len, use_bias=False, padding='same')\n",
    "    G_z = tf.identity(G_z, name='G_z')\n",
    "\n",
    "    # Flatten batch\n",
    "    nch = int(G_z.get_shape()[-1])\n",
    "    G_z_padded = tf.pad(G_z, [[0, 0], [0, flat_pad], [0, 0]])\n",
    "    G_z_flat = tf.reshape(G_z_padded, [-1, nch], name='G_z_flat')\n",
    "\n",
    "    # Encode to int16\n",
    "    def float_to_int16(x, name=None):\n",
    "        x_int16 = x * 32767.\n",
    "        x_int16 = tf.clip_by_value(x_int16, -32767., 32767.)\n",
    "        x_int16 = tf.cast(x_int16, tf.int16, name=name)\n",
    "        return x_int16\n",
    "    G_z_int16 = float_to_int16(G_z, name='G_z_int16')\n",
    "    G_z_flat_int16 = float_to_int16(G_z_flat, name='G_z_flat_int16')\n",
    "\n",
    "    # Create saver\n",
    "    G_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='G')\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    saver = tf.train.Saver(G_vars + [global_step])\n",
    "\n",
    "    # Export graph\n",
    "    tf.train.write_graph(tf.get_default_graph(), infer_dir, 'infer.pbtxt')\n",
    "\n",
    "    # Export MetaGraph\n",
    "    infer_metagraph_fp = os.path.join(infer_dir, 'infer.meta')\n",
    "    tf.train.export_meta_graph(\n",
    "        filename=infer_metagraph_fp,\n",
    "        clear_devices=True,\n",
    "        saver_def=saver.as_saver_def())\n",
    "\n",
    "    # Reset graph (in case training afterwards)\n",
    "    tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Generates a preview audio file every time a checkpoint is saved\n",
    "\"\"\"\n",
    "def preview(args):\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.io.wavfile import write as wavwrite\n",
    "    from scipy.signal import freqz\n",
    "\n",
    "    preview_dir = os.path.join(args.train_dir, 'preview')\n",
    "    if not os.path.isdir(preview_dir):\n",
    "        os.makedirs(preview_dir)\n",
    "\n",
    "    # Load graph\n",
    "    infer_metagraph_fp = os.path.join(args.train_dir, 'infer', 'infer.meta')\n",
    "    graph = tf.get_default_graph()\n",
    "    saver = tf.train.import_meta_graph(infer_metagraph_fp)\n",
    "\n",
    "    # Generate or restore z_i and z_o\n",
    "    z_fp = os.path.join(preview_dir, 'z.pkl')\n",
    "    if os.path.exists(z_fp):\n",
    "        with open(z_fp, 'rb') as f:\n",
    "            _zs = pickle.load(f)\n",
    "    else:\n",
    "        # Sample z\n",
    "        samp_feeds = {}\n",
    "        samp_feeds[graph.get_tensor_by_name('samp_z_n:0')] = args.preview_n\n",
    "        samp_fetches = {}\n",
    "        samp_fetches['zs'] = graph.get_tensor_by_name('samp_z:0')\n",
    "        with tf.Session() as sess:\n",
    "            _samp_fetches = sess.run(samp_fetches, samp_feeds)\n",
    "        _zs = _samp_fetches['zs']\n",
    "\n",
    "        # Save z\n",
    "        with open(z_fp, 'wb') as f:\n",
    "            pickle.dump(_zs, f)\n",
    "\n",
    "    # Set up graph for generating preview images\n",
    "    feeds = {}\n",
    "    feeds[graph.get_tensor_by_name('z:0')] = _zs\n",
    "    feeds[graph.get_tensor_by_name('flat_pad:0')] = int(args.data_sample_rate / 2)\n",
    "    fetches = {}\n",
    "    fetches['step'] = tf.train.get_or_create_global_step()\n",
    "    fetches['G_z'] = graph.get_tensor_by_name('G_z:0')\n",
    "    fetches['G_z_flat_int16'] = graph.get_tensor_by_name('G_z_flat_int16:0')\n",
    "    if args.wavegan_genr_pp:\n",
    "        fetches['pp_filter'] = graph.get_tensor_by_name('G/pp_filt/conv1d/kernel:0')[:, 0, 0]\n",
    "\n",
    "    # Summarize\n",
    "    G_z = graph.get_tensor_by_name('G_z_flat:0')\n",
    "    summaries = [\n",
    "      tf.summary.audio('preview', tf.expand_dims(G_z, axis=0), args.data_sample_rate, max_outputs=1)\n",
    "    ]\n",
    "    fetches['summaries'] = tf.summary.merge(summaries)\n",
    "    summary_writer = tf.summary.FileWriter(preview_dir)\n",
    "\n",
    "    # PP Summarize\n",
    "    if args.wavegan_genr_pp:\n",
    "        pp_fp = tf.placeholder(tf.string, [])\n",
    "        pp_bin = tf.read_file(pp_fp)\n",
    "        pp_png = tf.image.decode_png(pp_bin)\n",
    "        pp_summary = tf.summary.image('pp_filt', tf.expand_dims(pp_png, axis=0))\n",
    "\n",
    "    # Loop, waiting for checkpoints\n",
    "    ckpt_fp = None\n",
    "    while True:\n",
    "        latest_ckpt_fp = tf.train.latest_checkpoint(args.train_dir)\n",
    "        if latest_ckpt_fp != ckpt_fp:\n",
    "            print('Preview: {}'.format(latest_ckpt_fp))\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                saver.restore(sess, latest_ckpt_fp)\n",
    "\n",
    "                _fetches = sess.run(fetches, feeds)\n",
    "\n",
    "                _step = _fetches['step']\n",
    "\n",
    "            preview_fp = os.path.join(preview_dir, '{}.wav'.format(str(_step).zfill(8)))\n",
    "            wavwrite(preview_fp, args.data_sample_rate, _fetches['G_z_flat_int16'])\n",
    "\n",
    "            summary_writer.add_summary(_fetches['summaries'], _step)\n",
    "\n",
    "            if args.wavegan_genr_pp:\n",
    "                w, h = freqz(_fetches['pp_filter'])\n",
    "\n",
    "                fig = plt.figure()\n",
    "                plt.title('Digital filter frequncy response')\n",
    "                ax1 = fig.add_subplot(111)\n",
    "\n",
    "                plt.plot(w, 20 * np.log10(abs(h)), 'b')\n",
    "                plt.ylabel('Amplitude [dB]', color='b')\n",
    "                plt.xlabel('Frequency [rad/sample]')\n",
    "\n",
    "                ax2 = ax1.twinx()\n",
    "                angles = np.unwrap(np.angle(h))\n",
    "                plt.plot(w, angles, 'g')\n",
    "                plt.ylabel('Angle (radians)', color='g')\n",
    "                plt.grid()\n",
    "                plt.axis('tight')\n",
    "\n",
    "                _pp_fp = os.path.join(preview_dir, '{}_ppfilt.png'.format(str(_step).zfill(8)))\n",
    "                plt.savefig(_pp_fp)\n",
    "\n",
    "                with tf.Session() as sess:\n",
    "                    _summary = sess.run(pp_summary, {pp_fp: _pp_fp})\n",
    "                    summary_writer.add_summary(_summary, _step)\n",
    "                \n",
    "            print('Done')\n",
    "\n",
    "            ckpt_fp = latest_ckpt_fp\n",
    "\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Computes inception score every time a checkpoint is saved\n",
    "\"\"\"\n",
    "def incept(args):\n",
    "    incept_dir = os.path.join(args.train_dir, 'incept')\n",
    "    if not os.path.isdir(incept_dir):\n",
    "        os.makedirs(incept_dir)\n",
    "\n",
    "    # Load GAN graph\n",
    "    gan_graph = tf.Graph()\n",
    "    with gan_graph.as_default():\n",
    "        infer_metagraph_fp = os.path.join(args.train_dir, 'infer', 'infer.meta')\n",
    "        gan_saver = tf.train.import_meta_graph(infer_metagraph_fp)\n",
    "        score_saver = tf.train.Saver(max_to_keep=1)\n",
    "    gan_z = gan_graph.get_tensor_by_name('z:0')\n",
    "    gan_G_z = gan_graph.get_tensor_by_name('G_z:0')[:, :, 0]\n",
    "    gan_step = gan_graph.get_tensor_by_name('global_step:0')\n",
    "\n",
    "    # Load or generate latents\n",
    "    z_fp = os.path.join(incept_dir, 'z.pkl')\n",
    "    if os.path.exists(z_fp):\n",
    "        with open(z_fp, 'rb') as f:\n",
    "            _zs = pickle.load(f)\n",
    "    else:\n",
    "        gan_samp_z_n = gan_graph.get_tensor_by_name('samp_z_n:0')\n",
    "        gan_samp_z = gan_graph.get_tensor_by_name('samp_z:0')\n",
    "        with tf.Session(graph=gan_graph) as sess:\n",
    "            _zs = sess.run(gan_samp_z, {gan_samp_z_n: args.incept_n})\n",
    "        with open(z_fp, 'wb') as f:\n",
    "            pickle.dump(_zs, f)\n",
    "\n",
    "    # Load classifier graph\n",
    "    incept_graph = tf.Graph()\n",
    "    with incept_graph.as_default():\n",
    "        incept_saver = tf.train.import_meta_graph(args.incept_metagraph_fp)\n",
    "    incept_x = incept_graph.get_tensor_by_name('x:0')\n",
    "    incept_preds = incept_graph.get_tensor_by_name('scores:0')\n",
    "    incept_sess = tf.Session(graph=incept_graph)\n",
    "    incept_saver.restore(incept_sess, args.incept_ckpt_fp)\n",
    "\n",
    "    # Create summaries\n",
    "    summary_graph = tf.Graph()\n",
    "    with summary_graph.as_default():\n",
    "        incept_mean = tf.placeholder(tf.float32, [])\n",
    "        incept_std = tf.placeholder(tf.float32, [])\n",
    "        summaries = [\n",
    "            tf.summary.scalar('incept_mean', incept_mean),\n",
    "            tf.summary.scalar('incept_std', incept_std)\n",
    "        ]\n",
    "        summaries = tf.summary.merge(summaries)\n",
    "    summary_writer = tf.summary.FileWriter(incept_dir)\n",
    "\n",
    "    # Loop, waiting for checkpoints\n",
    "    ckpt_fp = None\n",
    "    _best_score = 0.\n",
    "    while True:\n",
    "        latest_ckpt_fp = tf.train.latest_checkpoint(args.train_dir)\n",
    "        if latest_ckpt_fp != ckpt_fp:\n",
    "            print('Incept: {}'.format(latest_ckpt_fp))\n",
    "\n",
    "            sess = tf.Session(graph=gan_graph)\n",
    "\n",
    "            gan_saver.restore(sess, latest_ckpt_fp)\n",
    "\n",
    "            _step = sess.run(gan_step)\n",
    "\n",
    "            _G_zs = []\n",
    "            for i in xrange(0, args.incept_n, 100):\n",
    "                _G_zs.append(sess.run(gan_G_z, {gan_z: _zs[i:i+100]}))\n",
    "            _G_zs = np.concatenate(_G_zs, axis=0)\n",
    "\n",
    "            _preds = []\n",
    "            for i in xrange(0, args.incept_n, 100):\n",
    "                _preds.append(incept_sess.run(incept_preds, {incept_x: _G_zs[i:i+100]}))\n",
    "            _preds = np.concatenate(_preds, axis=0)\n",
    "\n",
    "            # Split into k groups\n",
    "            _incept_scores = []\n",
    "            split_size = args.incept_n // args.incept_k\n",
    "            for i in xrange(args.incept_k):\n",
    "                _split = _preds[i * split_size:(i + 1) * split_size]\n",
    "                _kl = _split * (np.log(_split) - np.log(np.expand_dims(np.mean(_split, 0), 0)))\n",
    "                _kl = np.mean(np.sum(_kl, 1))\n",
    "                _incept_scores.append(np.exp(_kl))\n",
    "\n",
    "            _incept_mean, _incept_std = np.mean(_incept_scores), np.std(_incept_scores)\n",
    "\n",
    "            # Summarize\n",
    "            with tf.Session(graph=summary_graph) as summary_sess:\n",
    "                _summaries = summary_sess.run(summaries, {incept_mean: _incept_mean, incept_std: _incept_std})\n",
    "            summary_writer.add_summary(_summaries, _step)\n",
    "\n",
    "            # Save\n",
    "            if _incept_mean > _best_score:\n",
    "                score_saver.save(sess, os.path.join(incept_dir, 'best_score'), _step)\n",
    "                _best_score = _incept_mean\n",
    "\n",
    "            sess.close()\n",
    "\n",
    "            print('Done')\n",
    "\n",
    "            ckpt_fp = latest_ckpt_fp\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    incept_sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    data_dir=\"./piano/train\"\n",
    "    train_dir=\"./piano/train\"\n",
    "    data_sample_rate=16000\n",
    "    data_slice_len=16384\n",
    "    data_num_channels=1\n",
    "    data_overlap_ratio=0.\n",
    "    data_first_slice=False\n",
    "    data_pad_end=False\n",
    "    data_normalize=False\n",
    "    data_fast_wav=False\n",
    "    data_prefetch_gpu_num=0\n",
    "    wavegan_latent_dim=100\n",
    "    wavegan_kernel_len=25\n",
    "    wavegan_dim=64\n",
    "    wavegan_batchnorm=False\n",
    "    wavegan_disc_nupdates=5\n",
    "    wavegan_loss='wgan-gp'\n",
    "    wavegan_genr_upsample='zeros'\n",
    "    wavegan_genr_pp=False\n",
    "    wavegan_genr_pp_len=512\n",
    "    wavegan_disc_phaseshuffle=2\n",
    "    train_batch_size=64\n",
    "    train_save_secs=300\n",
    "    train_summary_secs=120\n",
    "    preview_n=32\n",
    "    incept_metagraph_fp='./eval/inception/infer.meta'\n",
    "    incept_ckpt_fp='./eval/inception/best_acc-103005'\n",
    "    incept_n=5000\n",
    "    incept_k=10\n",
    "    \n",
    "    wavegan_g_kwargs = {\n",
    "    'slice_len': data_slice_len,\n",
    "    'nch': data_num_channels,\n",
    "    'kernel_len': wavegan_kernel_len,\n",
    "    'dim': wavegan_dim,\n",
    "    'use_batchnorm': wavegan_batchnorm,\n",
    "    'upsample': wavegan_genr_upsample\n",
    "    }\n",
    "    \n",
    "    wavegan_d_kwargs = {\n",
    "    'kernel_len': wavegan_kernel_len,\n",
    "    'dim': wavegan_dim,\n",
    "    'use_batchnorm': wavegan_batchnorm,\n",
    "    'phaseshuffle_rad': wavegan_disc_phaseshuffle\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./piano/train\n"
     ]
    }
   ],
   "source": [
    "print (args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = glob.glob(os.path.join(args.data_dir, '*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 audio files in specified directory\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable G/z_project/dense/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-9e66f9447c6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Did not find any audio files in specified directory'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found {} audio files in specified directory'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0minfer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-7ae55c4df987>\u001b[0m in \u001b[0;36minfer\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Execute generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'G'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mG_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWaveGANGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwavegan_g_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwavegan_genr_pp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pp_filt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-ea00d898fb18>\u001b[0m in \u001b[0;36mWaveGANGenerator\u001b[1;34m(z, slice_len, nch, kernel_len, dim, use_batchnorm, upsample, train)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'z_project'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdim_mul\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdim_mul\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatchnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\u001b[0m in \u001b[0;36mdense\u001b[1;34m(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0m_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 _reuse=reuse)\n\u001b[1;32m--> 188\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1225\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m     \"\"\"\n\u001b[1;32m-> 1227\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_subclass_implementers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[1;31m# Build layer if applicable (if the `build` method has been overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 538\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m         \u001b[1;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;31m# constrained to set self.built.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1601\u001b[0m     \u001b[1;31m# Only call `build` if the user has manually overridden the build method.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1602\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_is_default'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1603\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1605\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m         trainable=True)\n\u001b[0m\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m       self.bias = self.add_weight(\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m             getter=vs.get_variable)\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         aggregation=aggregation)\n\u001b[0m\u001b[0;32m    350\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[1;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[0;32m    605\u001b[0m     new_variable = getter(\n\u001b[0;32m    606\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m         **kwargs_for_getter)\n\u001b[0m\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m     \u001b[1;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1479\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1220\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    545\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    497\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[1;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    846\u001b[0m         \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"tensorflow/python\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(\n\u001b[1;32m--> 848\u001b[1;33m             traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable G/z_project/dense/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"c:\\users\\chris\\appdata\\local\\conda\\conda\\envs\\pia\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "if len(fps) == 0:\n",
    "    raise Exception('Did not find any audio files in specified directory')\n",
    "print('Found {} audio files in specified directory'.format(len(fps)))\n",
    "infer(args)\n",
    "train(fps, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html\n",
    "SoundDataset\n",
    "\n",
    "https://arxiv.org/pdf/1802.04208.pdf\n",
    "waveGAN Paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
