{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveGAN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Teammember |                    |\n",
    "|------------|--------------------|\n",
    "| 1.         | Christopher Caldwell |\n",
    "| 2.         | Fabian MÃ¼ller      |\n",
    "| 3.         | An Dang         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The code in this notebook has been taken almost completely from the orginial waveGAN github repository (Donahue et. al., 2019) <sup>[1]</sup>, which can be found [here](https://github.com/chrisdonahue/wavegan). The code has been conformed to fit the jupyter notebook.  \n",
    "\n",
    "For the purpose of this lecture, we will first give a complete overview of the waveGAN implementation, the idea behind it, and the challenges we had to face.  \n",
    "We will continue to intensively investigate how and why certain measures have been taken in between each code block. Some improvements from our side will be considered in theory.  \n",
    "At the end the results of the waveGAN training are displayed and the corresponding results are reviewed and interpreted. Related and further implementations for music generation are presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Motivation & Idea\n",
    "The idea behind waveGAN is to use raw audio as a source for unsupervised training data to create music using GANs with a very similar approach to [DCGAN (Radford et. al., 2016)](https://arxiv.org/pdf/1511.06434.pdf)<sup>[2]</sup>. WaveGAN uses a generator network to generate music which the discriminator network cannot distinguish from real music.  \n",
    "This approach is very similar to the image-generating GAN networks that have become known around the world. In waveGAN, however, the data source is not an RGB image, but music with a high sampling rate.  \n",
    "Music data is a sequential type of data and must therefore take the dimension of time into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "#### Loading the data\n",
    "\n",
    "The most efficient way to load the data into the waveGAN neural network is to start off with *raw PCM 16-bit 16kHZ mono music data.*  \n",
    "For this implementation we used the [Bach piano performances](http://deepyeti.ucsd.edu/cdonahue/wavegan/data/mancini_piano.tar.gz) training set<sup>[3]</sup>, specifically gathered for waveGAN by its author C. Donahue. This music dataset is however 24 bit, stereo 48kHZ data. For waveGan to work with the music, it must be converted to 16 bit.  \n",
    "The waveGAN implementation can however work with the 48kHz sampling rate and stereo mode. It internally converts the sampling rate to 16kHz using the scipy module and the number of channels can be determined through defining the according hyperparameter before training. The implementation can also handle mp3 data, by using the librosa module to convert it to the required dataformat.\n",
    "But to save computation time, we decided to convert the original data with the open-source programm [Audacity](https://www.audacity.de/) in advance to the optimal *raw PCM 16-bit 16kHZ mono music data.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read as wavread\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preping the Data\n",
    "\n",
    "## Decode Audio\n",
    "the decode audio function decodes the audio file paths into 32-bit floating point vectors.  \n",
    "The input parameters are:  \n",
    "\n",
    "| args         | description                                                 |\n",
    "|--------------|-------------------------------------------------------------|\n",
    "| fp           | a string containing the filepath to the WAV file (required) |\n",
    "| fs           | resamples the the decoded audio to this sampling rate       |\n",
    "| num_channels | specify the number of channels of the music data            |\n",
    "| normalize    | normalize the decoded music data                            |\n",
    "| fast_wav     | specify if the source data can be processed with the faster scipy module or if it needs librosa to convert the file.|\n",
    "\n",
    "the function returns a np.float32 array containing the audio samples at the specific sample rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(fp, fs=None, num_channels=1, normalize=False, fast_wav=False):\n",
    "    \n",
    "    # Use scipy fast wave read method, if fast_wav is true\n",
    "    if fast_wav:\n",
    "        \n",
    "        # Read with scipy wavread (fast).\n",
    "        _fs, _wav = wavread(fp)\n",
    "        \n",
    "        # if the sample rate is specified, but not equal to the return sample rate from the wavread method. a notimplementederror is raised.\n",
    "        # use scipy.signal.resample to resample the \n",
    "        if fs is not None and fs != _fs:\n",
    "            ## NOT ORIGINAL WAVEGAN IMPLEMENTATION: \n",
    "            ## added resampling code as an alternative\n",
    "            \"\"\"\n",
    "            from scipy.signal import resample\n",
    "            _wav = resample(_wav, fs)\n",
    "            \"\"\" \n",
    "            raise NotImplementedError('Scipy cannot resample audio.')\n",
    "        \n",
    "        # Convert _wav to float32 if the datatype is int16\n",
    "        if _wav.dtype == np.int16:\n",
    "            _wav = _wav.astype(np.float32)\n",
    "            _wav /= 32768.\n",
    "        \n",
    "        # _wav file is of type float32.\n",
    "        elif _wav.dtype == np.float32:\n",
    "            _wav = np.copy(_wav)\n",
    "        \n",
    "        # if _wav file is not of type float32 or int16 the wav file cannot be processed with scipy. \n",
    "        # fast_wav is not possible with this file\n",
    "        else:\n",
    "              raise NotImplementedError('Scipy cannot process atypical WAV files.')\n",
    "    \n",
    "    # if fast_wav is false, the source music data will be processed with the librosa module\n",
    "    else:\n",
    "        # Decode with librosa load (slow but supports file formats like mp3).\n",
    "        import librosa\n",
    "        \n",
    "        mono = False\n",
    "        \"\"\"\n",
    "        # check if the music source file is mono.\n",
    "        if num_channels > 1:\n",
    "            mono = False\n",
    "        elif num_channels == 1:\n",
    "            mono = True\n",
    "        else:\n",
    "            raise NotImplementedError('Librosa cannot process WAV files with less than one channel')\n",
    "        \"\"\"\n",
    "        \n",
    "        _wav, _fs = librosa.core.load(fp, sr=fs, mono=mono)\n",
    "        \n",
    "        \n",
    "        if _wav.ndim == 2:\n",
    "            _wav = np.swapaxes(_wav, 0, 1)\n",
    "\n",
    "    \n",
    "    # stop if datatype of wav is not float32 at this point.\n",
    "    assert _wav.dtype == np.float32\n",
    "\n",
    "    # At this point, _wav is np.float32 either [nsamps,] or [nsamps, nch].\n",
    "    # We want [nsamps, 1, nch] to mimic 2D shape of spectral feats.\n",
    "    if _wav.ndim == 1:\n",
    "        nsamps = _wav.shape[0]\n",
    "        nch = 1\n",
    "    else:\n",
    "        nsamps, nch = _wav.shape\n",
    "    _wav = np.reshape(_wav, [nsamps, 1, nch])\n",
    " \n",
    "    # Average (mono) or expand (stereo) channels\n",
    "    if nch != num_channels:\n",
    "        if num_channels == 1:\n",
    "            _wav = np.mean(_wav, 2, keepdims=True)\n",
    "        elif nch == 1 and num_channels == 2:\n",
    "            _wav = np.concatenate([_wav, _wav], axis=2)\n",
    "        else:\n",
    "            raise ValueError('Number of audio channels not equal to num specified')\n",
    "\n",
    "    # if specified the data is normalized.\n",
    "    if normalize:\n",
    "        factor = np.max(np.abs(_wav))\n",
    "        if factor > 0:\n",
    "            _wav /= factor\n",
    "\n",
    "    return _wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode Extract and Batch\n",
    "\n",
    "The decode extract and batch function takes the file paths and initializes batches to be fed to the neural network.  \n",
    "Depending on the specified parameters (repeat | shuffle | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_extract_and_batch(fps, batch_size, slice_len, decode_fs, \n",
    "                             decode_num_channels, decode_normalize=True,\n",
    "                             decode_fast_wav=False, decode_parallel_calls=1,\n",
    "                             slice_randomize_offset=False, slice_first_only=False,\n",
    "                             slice_overlap_ratio=0, slice_pad_end=False,\n",
    "                             repeat=False,\n",
    "                             shuffle=False,\n",
    "                             shuffle_buffer_size=None,\n",
    "                             prefetch_size=None,\n",
    "                             prefetch_gpu_num=None):\n",
    "    \"\"\"Decodes audio file paths into mini-batches of samples.\n",
    "    Args:\n",
    "        fps: List of audio file paths.\n",
    "        batch_size: Number of items in the batch.\n",
    "        slice_len: Length of the sliceuences in samples or feature timesteps.\n",
    "        decode_fs: (Re-)sample rate for decoded audio files.\n",
    "        decode_num_channels: Number of channels for decoded audio files.\n",
    "        decode_normalize: If false, do not normalize audio waveforms.\n",
    "        decode_fast_wav: If true, uses scipy to decode standard wav files.\n",
    "        decode_parallel_calls: Number of parallel decoding threads.\n",
    "        slice_randomize_offset: If true, randomize starting position for slice.\n",
    "        slice_first_only: If true, only use first slice from each audio file.\n",
    "        slice_overlap_ratio: Ratio of overlap between adjacent slices.\n",
    "        slice_pad_end: If true, allows zero-padded examples from the end of each audio file.\n",
    "        repeat: If true (for training), continuously iterate through the dataset.\n",
    "        shuffle: If true (for training), buffer and shuffle the sliceuences.\n",
    "        shuffle_buffer_size: Number of examples to queue up before grabbing a batch.\n",
    "        prefetch_size: Number of examples to prefetch from the queue.\n",
    "        prefetch_gpu_num: If specified, prefetch examples to GPU.\n",
    "    Returns:\n",
    "        A tuple of np.float32 tensors representing audio waveforms.\n",
    "        audio: [batch_size, slice_len, 1, nch]\n",
    "    \"\"\"\n",
    "  \n",
    "    # Create dataset of filepaths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(fps)\n",
    "\n",
    "    # Shuffle all filepaths every epoch\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(fps))\n",
    "\n",
    "    # Repeat\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    \n",
    "    # Prepare the transformation for each dataelement in a definition\n",
    "    def _decode_audio_shaped(fp):\n",
    "        _decode_audio_closure = lambda _fp: decode_audio(_fp, \n",
    "                                                         fs=decode_fs,\n",
    "                                                         num_channels=decode_num_channels,\n",
    "                                                         normalize=decode_normalize,\n",
    "                                                         fast_wav=decode_fast_wav)\n",
    "        \n",
    "        # tf.py_func: Wraps a python function and uses it as a TensorFlow op\n",
    "        audio = tf.py_func(_decode_audio_closure,\n",
    "                           [fp],\n",
    "                           tf.float32,\n",
    "                           stateful=False)\n",
    "        \n",
    "        audio.set_shape([None, 1, decode_num_channels])\n",
    "\n",
    "        return audio\n",
    "    \n",
    "    # dataset.map()\n",
    "    # This transformation applies map_func to each element of this dataset, and returns a new dataset \n",
    "    # containing the transformed elements, in the same order as they appeared in the input.\n",
    "    # Decode audio by using \"_decode_audio_shaped\" as the transformation function\n",
    "    # num_parallel_calls specifies the number of parallel decode threads\n",
    "    dataset = dataset.map(_decode_audio_shaped, num_parallel_calls=decode_parallel_calls)\n",
    "\n",
    "    # Parallel\n",
    "    def _slice(audio):\n",
    "        # Calculate hop size\n",
    "        if slice_overlap_ratio < 0:\n",
    "            raise ValueError('Overlap ratio must be greater than 0')\n",
    "        slice_hop = int(round(slice_len * (1. - slice_overlap_ratio)) + 1e-4)\n",
    "        if slice_hop < 1:\n",
    "            raise ValueError('Overlap ratio too high')\n",
    "\n",
    "        # Randomize starting phase:\n",
    "        if slice_randomize_offset:\n",
    "            start = tf.random_uniform([], maxval=slice_len, dtype=tf.int32)\n",
    "            audio = audio[start:]\n",
    "\n",
    "        # Extract sliceuences\n",
    "        audio_slices = tf.contrib.signal.frame(audio, \n",
    "                                               slice_len,\n",
    "                                               slice_hop,\n",
    "                                               pad_end=slice_pad_end,\n",
    "                                               pad_value=0,\n",
    "                                               axis=0)\n",
    "\n",
    "        # Only use first slice if requested\n",
    "        if slice_first_only:\n",
    "            audio_slices = audio_slices[:1]\n",
    "\n",
    "        return audio_slices\n",
    "\n",
    "    def _slice_dataset_wrapper(audio):\n",
    "        audio_slices = _slice(audio)\n",
    "        return tf.data.Dataset.from_tensor_slices(audio_slices)\n",
    "\n",
    "    # Extract parallel sliceuences from both audio and features\n",
    "    dataset = dataset.flat_map(_slice_dataset_wrapper)\n",
    "\n",
    "    # Shuffle examples\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Make batches\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # Prefetch a number of batches\n",
    "    if prefetch_size is not None:\n",
    "        dataset = dataset.prefetch(prefetch_size)\n",
    "        if prefetch_gpu_num is not None and prefetch_gpu_num >= 0:\n",
    "            dataset = dataset.apply(tf.data.experimental.prefetch_to_device('/device:GPU:{}'.format(prefetch_gpu_num)))\n",
    "\n",
    "    # Get tensors\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "  \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_transpose(inputs, filters, kernel_width, stride=4, padding='same', upsample='zeros'):\n",
    "    if upsample == 'zeros':\n",
    "        return tf.layers.conv2d_transpose(tf.expand_dims(inputs, axis=1), \n",
    "                                          filters,\n",
    "                                          (1, kernel_width),\n",
    "                                          strides=(1, stride),\n",
    "                                          padding='same')[:, 0]\n",
    "    \n",
    "    # If Upsampling should use nearest neighbor\n",
    "    elif upsample == 'nn':\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        _, w, nch = inputs.get_shape().as_list()\n",
    "\n",
    "        x = inputs\n",
    "\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = tf.image.resize_nearest_neighbor(x, [1, w * stride])\n",
    "        x = x[:, 0]\n",
    "\n",
    "        return tf.layers.conv1d(x, filters, kernel_width, 1, padding='same')\n",
    "  \n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(inputs, alpha=0.2):\n",
    "    return tf.maximum(alpha * inputs, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Phaseshuffle\n",
    "\n",
    "The upscaling by convolution is known to create checkerboard artefacts:\n",
    "![Checkerboard Artefacts](./checkerboard.png)\n",
    "<sup>[9]</sup> [Source: Checkerboard Artefacts](http://physhik.com/waveGAN/)  \n",
    "\n",
    "The upscaling artefacts not only appear with images. During the upsacling process sounddata generates sounddata. To overcome these artefacts, waveGAN uses Phaseshuffle.\n",
    "\n",
    "![Phaseshuffle](./phaseshuffle.png)\n",
    "<sup>[9]</sup> [Source: Phase Shuffle](https://arxiv.org/pdf/1802.04208.pdf)  \n",
    "\n",
    "At each layer of the discriminator, the phase of each channel is shuffled, using a uniformely distributed random value between the specified `wavegan_disc_phaseshuffle` hyperparameter. That way, the discriminator can not simply learn to recognize the artifacts to distinguish between real and fake data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_phaseshuffle(x, rad, pad_type='reflect'):\n",
    "    b, x_len, nch = x.get_shape().as_list()\n",
    "\n",
    "    phase = tf.random_uniform([], minval=-rad, maxval=rad + 1, dtype=tf.int32)\n",
    "    pad_l = tf.maximum(phase, 0)\n",
    "    pad_r = tf.maximum(-phase, 0)\n",
    "    phase_start = pad_r\n",
    "    x = tf.pad(x, [[0, 0], [pad_l, pad_r], [0, 0]], mode=pad_type)\n",
    "\n",
    "    x = x[:, phase_start:phase_start+x_len]\n",
    "    x.set_shape([b, x_len, nch])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveGAN Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WaveGAN Discriminator Architechture](./discriminator.png)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `input layer` of the discriminator is either a sample of real music or generated music from the generator network.\n",
    "The shape of the input layer consists of  \n",
    "*n: batchsize*  \n",
    "*samplesize* <sup>(16384 ~ 1 sec at 16kHz | 32768 ~ 2 sec at 16 kHz | 65536 ~ 4 sec at 16 kHz)</sup>  \n",
    "*c: number of channels*  \n",
    "\n",
    "\n",
    "Following the Input Layer come 4 repetitions of convolutional layers, combined with a Linear ReLU activation function and the phase shuffling operations.  \n",
    "After the fourth repetition, a 5 repetition occurs, with a reshape instead of the phase-shuffling.  \n",
    "Each convloution operation has the following parameters:  \n",
    "\n",
    "* Stride = 4\n",
    "* Kernelsize = 25 (1 dimensional)\n",
    "\n",
    "At the end, a single Dense Layer with a \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kernel Length of 25 (5x5)\n",
    "\n",
    "def WaveGANDiscriminator(X, reuse_vars=None, kernel_len=25, dim=64, use_batchnorm=False, phaseshuffle_rad=0):\n",
    "  \n",
    "    batch_size = tf.shape(X)[0]\n",
    "    slice_len = int(X.get_shape()[1])\n",
    "\n",
    "    \n",
    "    if use_batchnorm:\n",
    "        batchnorm = lambda x: tf.layers.batch_normalization(x, training=True)\n",
    "    else:\n",
    "        batchnorm = lambda x: x\n",
    "\n",
    "    if phaseshuffle_rad > 0:\n",
    "        phaseshuffle = lambda x: apply_phaseshuffle(x, phaseshuffle_rad)\n",
    "    else:\n",
    "        phaseshuffle = lambda x: x\n",
    "\n",
    "        \n",
    "    # Layer 0\n",
    "    # [16384, 1] -> [4096, 64]\n",
    "    output = X\n",
    "    \n",
    "    with tf.variable_scope('downconv_0'):\n",
    "        output = tf.layers.conv1d(output, dim, kernel_len, 4, padding='SAME')\n",
    "    output = lrelu(output)\n",
    "    output = phaseshuffle(output)\n",
    "\n",
    "    # Layer 1\n",
    "    # [4096, 64] -> [1024, 128]\n",
    "    with tf.variable_scope('downconv_1'):\n",
    "        output = tf.layers.conv1d(output, dim * 2, kernel_len, 4, padding='SAME')\n",
    "        output = batchnorm(output)\n",
    "    output = lrelu(output)\n",
    "    output = phaseshuffle(output)\n",
    "\n",
    "    # Layer 2\n",
    "    # [1024, 128] -> [256, 256]\n",
    "    with tf.variable_scope('downconv_2'):\n",
    "        output = tf.layers.conv1d(output, dim * 4, kernel_len, 4, padding='SAME')\n",
    "        output = batchnorm(output)\n",
    "    output = lrelu(output)\n",
    "    output = phaseshuffle(output)\n",
    "\n",
    "    # Layer 3\n",
    "    # [256, 256] -> [64, 512]\n",
    "    with tf.variable_scope('downconv_3'):\n",
    "        output = tf.layers.conv1d(output, dim * 8, kernel_len, 4, padding='SAME')\n",
    "        output = batchnorm(output)\n",
    "    output = lrelu(output)\n",
    "    output = phaseshuffle(output)\n",
    "\n",
    "    # Layer 4\n",
    "    # [64, 512] -> [16, 1024]\n",
    "    with tf.variable_scope('downconv_4'):\n",
    "        output = tf.layers.conv1d(output, dim * 16, kernel_len, 4, padding='SAME')\n",
    "        output = batchnorm(output)\n",
    "    output = lrelu(output)\n",
    "\n",
    "    # Two seconds -> 16384 samples / second ---> 32768 / 2 seconds\n",
    "    if slice_len == 32768:\n",
    "        # Layer 5\n",
    "        # [32, 1024] -> [16, 2048]\n",
    "        with tf.variable_scope('downconv_5'):\n",
    "            output = tf.layers.conv1d(output, dim * 32, kernel_len, 2, padding='SAME')\n",
    "            output = batchnorm(output)\n",
    "        output = lrelu(output)\n",
    "    \n",
    "    # Four seconds -> 16384 samples / second ---> 65536 / 4 seconds\n",
    "    elif slice_len == 65536:\n",
    "        # Layer 5\n",
    "        # [64, 1024] -> [16, 2048]\n",
    "        with tf.variable_scope('downconv_5'):\n",
    "            output = tf.layers.conv1d(output, dim * 32, kernel_len, 4, padding='SAME')\n",
    "            output = batchnorm(output)\n",
    "        output = lrelu(output)\n",
    "\n",
    "    # Flatten\n",
    "    output = tf.reshape(output, [batch_size, -1])\n",
    "\n",
    "    # Connect to single logit\n",
    "    with tf.variable_scope('output'):\n",
    "        output = tf.layers.dense(output, 1)[:, 0]\n",
    "\n",
    "    # Don't need to aggregate batchnorm update ops like we do for the generator because we only use the discriminator for training\n",
    "\n",
    "    return output\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WaveGANGenerator(z, slice_len=16384, nch=1, kernel_len=25, dim=64, use_batchnorm=False, upsample='zeros', train=False):\n",
    "\n",
    "    assert slice_len in [16384, 32768, 65536]\n",
    "    batch_size = tf.shape(z)[0]\n",
    "\n",
    "    if use_batchnorm:\n",
    "        batchnorm = lambda x: tf.layers.batch_normalization(x, training=train)\n",
    "    else:\n",
    "        batchnorm = lambda x: x\n",
    "\n",
    "    # FC and reshape for convolution\n",
    "    # [100] -> [16, 1024]\n",
    "    dim_mul = 16 if slice_len == 16384 else 32\n",
    "    output = z\n",
    "    with tf.variable_scope('z_project'):\n",
    "        output = tf.layers.dense(output, 4 * 4 * dim * dim_mul)\n",
    "        output = tf.reshape(output, [batch_size, 16, dim * dim_mul])\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "    dim_mul //= 2\n",
    "\n",
    "    # Layer 0\n",
    "    # [16, 1024] -> [64, 512]\n",
    "    with tf.variable_scope('upconv_0'):\n",
    "        output = conv1d_transpose(output, dim * dim_mul, kernel_len, 4, upsample=upsample)\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "    dim_mul //= 2\n",
    "\n",
    "    # Layer 1\n",
    "    # [64, 512] -> [256, 256]\n",
    "    with tf.variable_scope('upconv_1'):\n",
    "        output = conv1d_transpose(output, dim * dim_mul, kernel_len, 4, upsample=upsample)\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "    dim_mul //= 2\n",
    "\n",
    "    # Layer 2\n",
    "    # [256, 256] -> [1024, 128]\n",
    "    with tf.variable_scope('upconv_2'):\n",
    "        output = conv1d_transpose(output, dim * dim_mul, kernel_len, 4, upsample=upsample)\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "    dim_mul //= 2\n",
    "\n",
    "    # Layer 3\n",
    "    # [1024, 128] -> [4096, 64]\n",
    "    with tf.variable_scope('upconv_3'):\n",
    "        output = conv1d_transpose(output, dim * dim_mul, kernel_len, 4, upsample=upsample)\n",
    "        output = batchnorm(output)\n",
    "    output = tf.nn.relu(output)\n",
    "\n",
    "    if slice_len == 16384:\n",
    "        # Layer 4\n",
    "        # [4096, 64] -> [16384, nch]\n",
    "        with tf.variable_scope('upconv_4'):\n",
    "            output = conv1d_transpose(output, nch, kernel_len, 4, upsample=upsample)\n",
    "        output = tf.nn.tanh(output)\n",
    "   \n",
    "    elif slice_len == 32768:\n",
    "        # Layer 4\n",
    "        # [4096, 128] -> [16384, 64]\n",
    "        with tf.variable_scope('upconv_4'):\n",
    "            output = conv1d_transpose(output, dim, kernel_len, 4, upsample=upsample)\n",
    "            output = batchnorm(output)\n",
    "        output = tf.nn.relu(output)\n",
    "\n",
    "        # Layer 5\n",
    "        # [16384, 64] -> [32768, nch]\n",
    "        with tf.variable_scope('upconv_5'):\n",
    "            output = conv1d_transpose(output, nch, kernel_len, 2, upsample=upsample)\n",
    "        output = tf.nn.tanh(output)\n",
    "        \n",
    "    elif slice_len == 65536:\n",
    "        # Layer 4\n",
    "        # [4096, 128] -> [16384, 64]\n",
    "        with tf.variable_scope('upconv_4'):\n",
    "            output = conv1d_transpose(output, dim, kernel_len, 4, upsample=upsample)\n",
    "            output = batchnorm(output)\n",
    "        output = tf.nn.relu(output)\n",
    "\n",
    "        # Layer 5\n",
    "        # [16384, 64] -> [65536, nch]\n",
    "        with tf.variable_scope('upconv_5'):\n",
    "            output = conv1d_transpose(output, nch, kernel_len, 4, upsample=upsample)\n",
    "        output = tf.nn.tanh(output)\n",
    "\n",
    "    # Automatically update batchnorm moving averages every time G is used during training\n",
    "    if train and use_batchnorm:\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=tf.get_variable_scope().name)\n",
    "        if slice_len == 16384:\n",
    "            assert len(update_ops) == 10\n",
    "        else:\n",
    "            assert len(update_ops) == 12\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            output = tf.identity(output)\n",
    "\n",
    "    return output\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding config for GPU - not using all of the GPUs Memory -> leading to crashes\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fps, args):\n",
    "    with tf.name_scope('loader'):\n",
    "        x = decode_extract_and_batch(\n",
    "            fps,\n",
    "            batch_size=args.train_batch_size,\n",
    "            slice_len=args.data_slice_len,\n",
    "            decode_fs=args.data_sample_rate,\n",
    "            decode_num_channels=args.data_num_channels,\n",
    "            decode_fast_wav=args.data_fast_wav,\n",
    "            decode_parallel_calls=4,\n",
    "            slice_randomize_offset=False if args.data_first_slice else True,\n",
    "            slice_first_only=args.data_first_slice,\n",
    "            slice_overlap_ratio=0. if args.data_first_slice else args.data_overlap_ratio,\n",
    "            slice_pad_end=True if args.data_first_slice else args.data_pad_end,\n",
    "            repeat=True,\n",
    "            shuffle=True,\n",
    "            shuffle_buffer_size=4096,\n",
    "            prefetch_size=args.train_batch_size * 4,\n",
    "            prefetch_gpu_num=args.data_prefetch_gpu_num)[:, :, 0]\n",
    "\n",
    "    # Make z vector\n",
    "    z = tf.random_uniform([args.train_batch_size, args.wavegan_latent_dim], -1., 1., dtype=tf.float32)\n",
    "\n",
    "    # Make generator\n",
    "    with tf.variable_scope('G'):\n",
    "        G_z = WaveGANGenerator(z, train=True, **args.wavegan_g_kwargs)\n",
    "        if args.wavegan_genr_pp:\n",
    "            with tf.variable_scope('pp_filt'):\n",
    "                G_z = tf.layers.conv1d(G_z, 1, args.wavegan_genr_pp_len, use_bias=False, padding='same')\n",
    "    G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='G')\n",
    "\n",
    "    # Print G summary\n",
    "    print('-' * 80)\n",
    "    print('Generator vars')\n",
    "    nparams = 0\n",
    "    for v in G_vars:\n",
    "        v_shape = v.get_shape().as_list()\n",
    "        v_n = reduce(lambda x, y: x * y, v_shape)\n",
    "        nparams += v_n\n",
    "        print('{} ({}): {}'.format(v.get_shape().as_list(), v_n, v.name))\n",
    "    print('Total params: {} ({:.2f} MB)'.format(nparams, (float(nparams) * 4) / (1024 * 1024)))\n",
    "\n",
    "    # Summarize\n",
    "    tf.summary.audio('x', x, args.data_sample_rate)\n",
    "    tf.summary.audio('G_z', G_z, args.data_sample_rate)\n",
    "    G_z_rms = tf.sqrt(tf.reduce_mean(tf.square(G_z[:, :, 0]), axis=1))\n",
    "    x_rms = tf.sqrt(tf.reduce_mean(tf.square(x[:, :, 0]), axis=1))\n",
    "    tf.summary.histogram('x_rms_batch', x_rms)\n",
    "    tf.summary.histogram('G_z_rms_batch', G_z_rms)\n",
    "    tf.summary.scalar('x_rms', tf.reduce_mean(x_rms))\n",
    "    tf.summary.scalar('G_z_rms', tf.reduce_mean(G_z_rms))\n",
    "\n",
    "  \n",
    "\n",
    "    # Make real discriminator\n",
    "    with tf.name_scope('D_x'), tf.variable_scope('D'):\n",
    "        D_x = WaveGANDiscriminator(x, **args.wavegan_d_kwargs)\n",
    "    D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='D')\n",
    "\n",
    "    # Print D summary\n",
    "    print('-' * 80)\n",
    "    print('Discriminator vars')\n",
    "    nparams = 0\n",
    "    for v in D_vars:\n",
    "        v_shape = v.get_shape().as_list()\n",
    "        v_n = reduce(lambda x, y: x * y, v_shape)\n",
    "        nparams += v_n\n",
    "        print('{} ({}): {}'.format(v.get_shape().as_list(), v_n, v.name))\n",
    "    print('Total params: {} ({:.2f} MB)'.format(nparams, (float(nparams) * 4) / (1024 * 1024)))\n",
    "    print('-' * 80)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    # Make fake discriminator\n",
    "    with tf.name_scope('D_G_z'), tf.variable_scope('D', reuse=True):\n",
    "        D_G_z = WaveGANDiscriminator(G_z, **args.wavegan_d_kwargs)\n",
    "\n",
    "        \n",
    "        \n",
    "    # Create loss\n",
    "    D_clip_weights = None\n",
    "    if args.wavegan_loss == 'dcgan':\n",
    "        fake = tf.zeros([args.train_batch_size], dtype=tf.float32)\n",
    "        real = tf.ones([args.train_batch_size], dtype=tf.float32)\n",
    "\n",
    "        G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          logits=D_G_z,\n",
    "          labels=real\n",
    "        ))\n",
    "\n",
    "        D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          logits=D_G_z,\n",
    "          labels=fake\n",
    "        ))\n",
    "        D_loss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          logits=D_x,\n",
    "          labels=real\n",
    "        ))\n",
    "\n",
    "        D_loss /= 2.\n",
    "  \n",
    "    elif args.wavegan_loss == 'lsgan':\n",
    "        G_loss = tf.reduce_mean((D_G_z - 1.) ** 2)\n",
    "        D_loss = tf.reduce_mean((D_x - 1.) ** 2)\n",
    "        D_loss += tf.reduce_mean(D_G_z ** 2)\n",
    "        D_loss /= 2.\n",
    "    elif args.wavegan_loss == 'wgan':\n",
    "        G_loss = -tf.reduce_mean(D_G_z)\n",
    "        D_loss = tf.reduce_mean(D_G_z) - tf.reduce_mean(D_x)\n",
    "\n",
    "        with tf.name_scope('D_clip_weights'):\n",
    "            clip_ops = []\n",
    "            for var in D_vars:\n",
    "                clip_bounds = [-.01, .01]\n",
    "                clip_ops.append(tf.assign(var,\n",
    "                                          tf.clip_by_value(var,\n",
    "                                                           clip_bounds[0], \n",
    "                                                           clip_bounds[1])\n",
    "                                         )\n",
    "                               )\n",
    "            D_clip_weights = tf.group(*clip_ops)\n",
    "            \n",
    "            \n",
    "    elif args.wavegan_loss == 'wgan-gp':\n",
    "        G_loss = -tf.reduce_mean(D_G_z)\n",
    "        D_loss = tf.reduce_mean(D_G_z) - tf.reduce_mean(D_x)\n",
    "\n",
    "        alpha = tf.random_uniform(shape=[args.train_batch_size, 1, 1], minval=0., maxval=1.)\n",
    "        differences = G_z - x\n",
    "        interpolates = x + (alpha * differences)\n",
    "        with tf.name_scope('D_interp'), tf.variable_scope('D', reuse=True):\n",
    "            D_interp = WaveGANDiscriminator(interpolates, **args.wavegan_d_kwargs)\n",
    "\n",
    "        LAMBDA = 10\n",
    "        gradients = tf.gradients(D_interp, [interpolates])[0]\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1, 2]))\n",
    "        gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2.)\n",
    "        D_loss += LAMBDA * gradient_penalty\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    tf.summary.scalar('G_loss', G_loss)\n",
    "    tf.summary.scalar('D_loss', D_loss)\n",
    "\n",
    "    # Create (recommended) optimizer\n",
    "    if args.wavegan_loss == 'dcgan':\n",
    "        G_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=2e-4,\n",
    "            beta1=0.5)\n",
    "        D_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=2e-4,\n",
    "            beta1=0.5)\n",
    "    elif args.wavegan_loss == 'lsgan':\n",
    "        G_opt = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=1e-4)\n",
    "        D_opt = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=1e-4)\n",
    "    elif args.wavegan_loss == 'wgan':\n",
    "        G_opt = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=5e-5)\n",
    "        D_opt = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=5e-5)\n",
    "    elif args.wavegan_loss == 'wgan-gp':\n",
    "        G_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=1e-4,\n",
    "            beta1=0.5,\n",
    "            beta2=0.9)\n",
    "        D_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=1e-4,\n",
    "            beta1=0.5,\n",
    "            beta2=0.9)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "  \n",
    "    # Create training ops\n",
    "    G_train_op = G_opt.minimize(G_loss, \n",
    "                                var_list=G_vars,\n",
    "                                global_step=tf.train.get_or_create_global_step())\n",
    "    D_train_op = D_opt.minimize(D_loss, var_list=D_vars)\n",
    "\n",
    "    # Run training\n",
    "    with tf.train.MonitoredTrainingSession(checkpoint_dir=args.train_dir,\n",
    "                                           save_checkpoint_secs=args.train_save_secs,\n",
    "                                           save_summaries_secs=args.train_summary_secs) as sess:\n",
    "        print('-' * 80)\n",
    "        print('Training has started. Please use \\'tensorboard --logdir={}\\' to monitor.'.format(args.train_dir))\n",
    "        while True:\n",
    "            # Train discriminator\n",
    "            for i in xrange(args.wavegan_disc_nupdates):\n",
    "                sess.run(D_train_op)\n",
    "\n",
    "            # Enforce Lipschitz constraint for WGAN\n",
    "            if D_clip_weights is not None:\n",
    "                sess.run(D_clip_weights)\n",
    "\n",
    "            # Train generator\n",
    "            sess.run(G_train_op)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "  Creates and saves a MetaGraphDef for simple inference\n",
    "  Tensors:\n",
    "    'samp_z_n' int32 []: Sample this many latent vectors\n",
    "    'samp_z' float32 [samp_z_n, latent_dim]: Resultant latent vectors\n",
    "    'z:0' float32 [None, latent_dim]: Input latent vectors\n",
    "    'flat_pad:0' int32 []: Number of padding samples to use when flattening batch to a single audio file\n",
    "    'G_z:0' float32 [None, slice_len, 1]: Generated outputs\n",
    "    'G_z_int16:0' int16 [None, slice_len, 1]: Same as above but quantizied to 16-bit PCM samples\n",
    "    'G_z_flat:0' float32 [None, 1]: Outputs flattened into single audio file\n",
    "    'G_z_flat_int16:0' int16 [None, 1]: Same as above but quantized to 16-bit PCM samples\n",
    "  Example usage:\n",
    "    import tensorflow as tf\n",
    "    tf.reset_default_graph()\n",
    "    saver = tf.train.import_meta_graph('infer.meta')\n",
    "    graph = tf.get_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "    saver.restore(sess, 'model.ckpt-10000')\n",
    "    z_n = graph.get_tensor_by_name('samp_z_n:0')\n",
    "    _z = sess.run(graph.get_tensor_by_name('samp_z:0'), {z_n: 10})\n",
    "    z = graph.get_tensor_by_name('G_z:0')\n",
    "    _G_z = sess.run(graph.get_tensor_by_name('G_z:0'), {z: _z})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(args):\n",
    "    infer_dir = os.path.join(args.train_dir, 'infer')\n",
    "    if not os.path.isdir(infer_dir):\n",
    "        os.makedirs(infer_dir)\n",
    "\n",
    "    # Subgraph that generates latent vectors\n",
    "    samp_z_n = tf.placeholder(tf.int32, [], name='samp_z_n')\n",
    "    samp_z = tf.random_uniform([samp_z_n, args.wavegan_latent_dim], -1.0, 1.0, dtype=tf.float32, name='samp_z')\n",
    "\n",
    "    # Input zo\n",
    "    z = tf.placeholder(tf.float32, [None, args.wavegan_latent_dim], name='z')\n",
    "    flat_pad = tf.placeholder(tf.int32, [], name='flat_pad')\n",
    "\n",
    "    # Execute generator\n",
    "    with tf.variable_scope('G'):\n",
    "        G_z = WaveGANGenerator(z, train=False, **args.wavegan_g_kwargs)\n",
    "        if args.wavegan_genr_pp:\n",
    "            with tf.variable_scope('pp_filt'):\n",
    "                G_z = tf.layers.conv1d(G_z, 1, args.wavegan_genr_pp_len, use_bias=False, padding='same')\n",
    "    G_z = tf.identity(G_z, name='G_z')\n",
    "\n",
    "    # Flatten batch\n",
    "    nch = int(G_z.get_shape()[-1])\n",
    "    G_z_padded = tf.pad(G_z, [[0, 0], [0, flat_pad], [0, 0]])\n",
    "    G_z_flat = tf.reshape(G_z_padded, [-1, nch], name='G_z_flat')\n",
    "\n",
    "    # Encode to int16\n",
    "    def float_to_int16(x, name=None):\n",
    "        x_int16 = x * 32767.\n",
    "        x_int16 = tf.clip_by_value(x_int16, -32767., 32767.)\n",
    "        x_int16 = tf.cast(x_int16, tf.int16, name=name)\n",
    "        return x_int16\n",
    "    G_z_int16 = float_to_int16(G_z, name='G_z_int16')\n",
    "    G_z_flat_int16 = float_to_int16(G_z_flat, name='G_z_flat_int16')\n",
    "\n",
    "    # Create saver\n",
    "    G_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='G')\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    saver = tf.train.Saver(G_vars + [global_step])\n",
    "\n",
    "    # Export graph\n",
    "    tf.train.write_graph(tf.get_default_graph(), infer_dir, 'infer.pbtxt')\n",
    "\n",
    "    # Export MetaGraph\n",
    "    infer_metagraph_fp = os.path.join(infer_dir, 'infer.meta')\n",
    "    tf.train.export_meta_graph(\n",
    "        filename=infer_metagraph_fp,\n",
    "        clear_devices=True,\n",
    "        saver_def=saver.as_saver_def())\n",
    "\n",
    "    # Reset graph (in case training afterwards)\n",
    "    tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Generates a preview audio file every time a checkpoint is saved\n",
    "\"\"\"\n",
    "def preview(args):\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.io.wavfile import write as wavwrite\n",
    "    from scipy.signal import freqz\n",
    "\n",
    "    preview_dir = os.path.join(args.train_dir, 'preview')\n",
    "    if not os.path.isdir(preview_dir):\n",
    "        os.makedirs(preview_dir)\n",
    "\n",
    "    # Load graph\n",
    "    infer_metagraph_fp = os.path.join(args.train_dir, 'infer', 'infer.meta')\n",
    "    graph = tf.get_default_graph()\n",
    "    saver = tf.train.import_meta_graph(infer_metagraph_fp)\n",
    "\n",
    "    # Generate or restore z_i and z_o\n",
    "    z_fp = os.path.join(preview_dir, 'z.pkl')\n",
    "    if os.path.exists(z_fp):\n",
    "        with open(z_fp, 'rb') as f:\n",
    "            _zs = pickle.load(f)\n",
    "    else:\n",
    "        # Sample z\n",
    "        samp_feeds = {}\n",
    "        samp_feeds[graph.get_tensor_by_name('samp_z_n:0')] = args.preview_n\n",
    "        samp_fetches = {}\n",
    "        samp_fetches['zs'] = graph.get_tensor_by_name('samp_z:0')\n",
    "        with tf.Session(config=config) as sess:\n",
    "            _samp_fetches = sess.run(samp_fetches, samp_feeds)\n",
    "        _zs = _samp_fetches['zs']\n",
    "\n",
    "        # Save z\n",
    "        with open(z_fp, 'wb') as f:\n",
    "            pickle.dump(_zs, f)\n",
    "\n",
    "    # Set up graph for generating preview images\n",
    "    feeds = {}\n",
    "    feeds[graph.get_tensor_by_name('z:0')] = _zs\n",
    "    feeds[graph.get_tensor_by_name('flat_pad:0')] = int(args.data_sample_rate / 2)\n",
    "    fetches = {}\n",
    "    fetches['step'] = tf.train.get_or_create_global_step()\n",
    "    fetches['G_z'] = graph.get_tensor_by_name('G_z:0')\n",
    "    fetches['G_z_flat_int16'] = graph.get_tensor_by_name('G_z_flat_int16:0')\n",
    "    if args.wavegan_genr_pp:\n",
    "        fetches['pp_filter'] = graph.get_tensor_by_name('G/pp_filt/conv1d/kernel:0')[:, 0, 0]\n",
    "\n",
    "    # Summarize\n",
    "    G_z = graph.get_tensor_by_name('G_z_flat:0')\n",
    "    summaries = [\n",
    "      tf.summary.audio('preview', tf.expand_dims(G_z, axis=0), args.data_sample_rate, max_outputs=1)\n",
    "    ]\n",
    "    fetches['summaries'] = tf.summary.merge(summaries)\n",
    "    summary_writer = tf.summary.FileWriter(preview_dir)\n",
    "\n",
    "    # PP Summarize\n",
    "    if args.wavegan_genr_pp:\n",
    "        pp_fp = tf.placeholder(tf.string, [])\n",
    "        pp_bin = tf.read_file(pp_fp)\n",
    "        pp_png = tf.image.decode_png(pp_bin)\n",
    "        pp_summary = tf.summary.image('pp_filt', tf.expand_dims(pp_png, axis=0))\n",
    "\n",
    "    # Loop, waiting for checkpoints\n",
    "    ckpt_fp = None\n",
    "    while True:\n",
    "        latest_ckpt_fp = tf.train.latest_checkpoint(args.train_dir)\n",
    "        if latest_ckpt_fp != ckpt_fp:\n",
    "            print('Preview: {}'.format(latest_ckpt_fp))\n",
    "\n",
    "            with tf.Session(config=config) as sess:\n",
    "                saver.restore(sess, latest_ckpt_fp)\n",
    "\n",
    "                _fetches = sess.run(fetches, feeds)\n",
    "\n",
    "                _step = _fetches['step']\n",
    "\n",
    "            preview_fp = os.path.join(preview_dir, '{}.wav'.format(str(_step).zfill(8)))\n",
    "            wavwrite(preview_fp, args.data_sample_rate, _fetches['G_z_flat_int16'])\n",
    "\n",
    "            summary_writer.add_summary(_fetches['summaries'], _step)\n",
    "\n",
    "            if args.wavegan_genr_pp:\n",
    "                w, h = freqz(_fetches['pp_filter'])\n",
    "\n",
    "                fig = plt.figure()\n",
    "                plt.title('Digital filter frequncy response')\n",
    "                ax1 = fig.add_subplot(111)\n",
    "\n",
    "                plt.plot(w, 20 * np.log10(abs(h)), 'b')\n",
    "                plt.ylabel('Amplitude [dB]', color='b')\n",
    "                plt.xlabel('Frequency [rad/sample]')\n",
    "\n",
    "                ax2 = ax1.twinx()\n",
    "                angles = np.unwrap(np.angle(h))\n",
    "                plt.plot(w, angles, 'g')\n",
    "                plt.ylabel('Angle (radians)', color='g')\n",
    "                plt.grid()\n",
    "                plt.axis('tight')\n",
    "\n",
    "                _pp_fp = os.path.join(preview_dir, '{}_ppfilt.png'.format(str(_step).zfill(8)))\n",
    "                plt.savefig(_pp_fp)\n",
    "\n",
    "                with tf.Session(config=config) as sess:\n",
    "                    _summary = sess.run(pp_summary, {pp_fp: _pp_fp})\n",
    "                    summary_writer.add_summary(_summary, _step)\n",
    "                \n",
    "            print('Done')\n",
    "\n",
    "            ckpt_fp = latest_ckpt_fp\n",
    "\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Computes inception score every time a checkpoint is saved\n",
    "\"\"\"\n",
    "def incept(args):\n",
    "    incept_dir = os.path.join(args.train_dir, 'incept')\n",
    "    if not os.path.isdir(incept_dir):\n",
    "        os.makedirs(incept_dir)\n",
    "\n",
    "    # Load GAN graph\n",
    "    gan_graph = tf.Graph()\n",
    "    with gan_graph.as_default():\n",
    "        infer_metagraph_fp = os.path.join(args.train_dir, 'infer', 'infer.meta')\n",
    "        gan_saver = tf.train.import_meta_graph(infer_metagraph_fp)\n",
    "        score_saver = tf.train.Saver(max_to_keep=1)\n",
    "    gan_z = gan_graph.get_tensor_by_name('z:0')\n",
    "    gan_G_z = gan_graph.get_tensor_by_name('G_z:0')[:, :, 0]\n",
    "    gan_step = gan_graph.get_tensor_by_name('global_step:0')\n",
    "\n",
    "    # Load or generate latents\n",
    "    z_fp = os.path.join(incept_dir, 'z.pkl')\n",
    "    if os.path.exists(z_fp):\n",
    "        with open(z_fp, 'rb') as f:\n",
    "            _zs = pickle.load(f)\n",
    "    else:\n",
    "        gan_samp_z_n = gan_graph.get_tensor_by_name('samp_z_n:0')\n",
    "        gan_samp_z = gan_graph.get_tensor_by_name('samp_z:0')\n",
    "        with tf.Session(graph=gan_graph) as sess:\n",
    "            _zs = sess.run(gan_samp_z, {gan_samp_z_n: args.incept_n})\n",
    "        with open(z_fp, 'wb') as f:\n",
    "            pickle.dump(_zs, f)\n",
    "\n",
    "    # Load classifier graph\n",
    "    incept_graph = tf.Graph()\n",
    "    with incept_graph.as_default():\n",
    "        incept_saver = tf.train.import_meta_graph(args.incept_metagraph_fp)\n",
    "    incept_x = incept_graph.get_tensor_by_name('x:0')\n",
    "    incept_preds = incept_graph.get_tensor_by_name('scores:0')\n",
    "    incept_sess = tf.Session(graph=incept_graph)\n",
    "    incept_saver.restore(incept_sess, args.incept_ckpt_fp)\n",
    "\n",
    "    # Create summaries\n",
    "    summary_graph = tf.Graph()\n",
    "    with summary_graph.as_default():\n",
    "        incept_mean = tf.placeholder(tf.float32, [])\n",
    "        incept_std = tf.placeholder(tf.float32, [])\n",
    "        summaries = [\n",
    "            tf.summary.scalar('incept_mean', incept_mean),\n",
    "            tf.summary.scalar('incept_std', incept_std)\n",
    "        ]\n",
    "        summaries = tf.summary.merge(summaries)\n",
    "    summary_writer = tf.summary.FileWriter(incept_dir)\n",
    "\n",
    "    # Loop, waiting for checkpoints\n",
    "    ckpt_fp = None\n",
    "    _best_score = 0.\n",
    "    while True:\n",
    "        latest_ckpt_fp = tf.train.latest_checkpoint(args.train_dir)\n",
    "        if latest_ckpt_fp != ckpt_fp:\n",
    "            print('Incept: {}'.format(latest_ckpt_fp))\n",
    "\n",
    "            sess = tf.Session(graph=gan_graph)\n",
    "\n",
    "            gan_saver.restore(sess, latest_ckpt_fp)\n",
    "\n",
    "            _step = sess.run(gan_step)\n",
    "\n",
    "            _G_zs = []\n",
    "            for i in xrange(0, args.incept_n, 100):\n",
    "                _G_zs.append(sess.run(gan_G_z, {gan_z: _zs[i:i+100]}))\n",
    "            _G_zs = np.concatenate(_G_zs, axis=0)\n",
    "\n",
    "            _preds = []\n",
    "            for i in xrange(0, args.incept_n, 100):\n",
    "                _preds.append(incept_sess.run(incept_preds, {incept_x: _G_zs[i:i+100]}))\n",
    "            _preds = np.concatenate(_preds, axis=0)\n",
    "\n",
    "            # Split into k groups\n",
    "            _incept_scores = []\n",
    "            split_size = args.incept_n // args.incept_k\n",
    "            for i in xrange(args.incept_k):\n",
    "                _split = _preds[i * split_size:(i + 1) * split_size]\n",
    "                _kl = _split * (np.log(_split) - np.log(np.expand_dims(np.mean(_split, 0), 0)))\n",
    "                _kl = np.mean(np.sum(_kl, 1))\n",
    "                _incept_scores.append(np.exp(_kl))\n",
    "\n",
    "            _incept_mean, _incept_std = np.mean(_incept_scores), np.std(_incept_scores)\n",
    "\n",
    "            # Summarize\n",
    "            with tf.Session(graph=summary_graph) as summary_sess:\n",
    "                _summaries = summary_sess.run(summaries, {incept_mean: _incept_mean, incept_std: _incept_std})\n",
    "            summary_writer.add_summary(_summaries, _step)\n",
    "\n",
    "            # Save\n",
    "            if _incept_mean > _best_score:\n",
    "                score_saver.save(sess, os.path.join(incept_dir, 'best_score'), _step)\n",
    "                _best_score = _incept_mean\n",
    "\n",
    "            sess.close()\n",
    "\n",
    "            print('Done')\n",
    "\n",
    "            ckpt_fp = latest_ckpt_fp\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    incept_sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    data_dir=\"./piano/train/16bit-1chan\"\n",
    "    train_dir=\"./train\"\n",
    "    data_sample_rate=16000\n",
    "    data_slice_len=65536\n",
    "    data_num_channels=1\n",
    "    data_overlap_ratio=0.\n",
    "    data_first_slice=False\n",
    "    data_pad_end=False\n",
    "    data_normalize=False\n",
    "    data_fast_wav=True\n",
    "    data_prefetch_gpu_num=0\n",
    "    wavegan_latent_dim=100\n",
    "    wavegan_kernel_len=25\n",
    "    wavegan_dim=64\n",
    "    wavegan_batchnorm=False\n",
    "    wavegan_disc_nupdates=5\n",
    "    wavegan_loss='wgan-gp'\n",
    "    wavegan_genr_upsample='zeros'\n",
    "    wavegan_genr_pp=True\n",
    "    wavegan_genr_pp_len=512\n",
    "    wavegan_disc_phaseshuffle=0\n",
    "    train_batch_size=64\n",
    "    train_save_secs=300\n",
    "    train_summary_secs=120\n",
    "    preview_n=32\n",
    "    incept_metagraph_fp='./eval/inception/infer.meta'\n",
    "    incept_ckpt_fp='./eval/inception/best_acc-103005'\n",
    "    incept_n=5000\n",
    "    incept_k=10\n",
    "    \n",
    "    wavegan_g_kwargs = {\n",
    "    'slice_len': data_slice_len,\n",
    "    'nch': data_num_channels,\n",
    "    'kernel_len': wavegan_kernel_len,\n",
    "    'dim': wavegan_dim,\n",
    "    'use_batchnorm': wavegan_batchnorm,\n",
    "    'upsample': wavegan_genr_upsample\n",
    "    }\n",
    "    \n",
    "    wavegan_d_kwargs = {\n",
    "    'kernel_len': wavegan_kernel_len,\n",
    "    'dim': wavegan_dim,\n",
    "    'use_batchnorm': wavegan_batchnorm,\n",
    "    'phaseshuffle_rad': wavegan_disc_phaseshuffle\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./piano/train/16bit-1chan\n"
     ]
    }
   ],
   "source": [
    "print (args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = glob.glob(os.path.join(args.data_dir, '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./piano/train/16bit-1chan\\\\00.wav', './piano/train/16bit-1chan\\\\01.wav', './piano/train/16bit-1chan\\\\02.wav', './piano/train/16bit-1chan\\\\03.wav', './piano/train/16bit-1chan\\\\04.wav', './piano/train/16bit-1chan\\\\05.wav', './piano/train/16bit-1chan\\\\06.wav', './piano/train/16bit-1chan\\\\07.wav', './piano/train/16bit-1chan\\\\08.wav', './piano/train/16bit-1chan\\\\09.wav', './piano/train/16bit-1chan\\\\10.wav', './piano/train/16bit-1chan\\\\11.wav', './piano/train/16bit-1chan\\\\12.wav', './piano/train/16bit-1chan\\\\13.wav', './piano/train/16bit-1chan\\\\14.wav', './piano/train/16bit-1chan\\\\15.wav', './piano/train/16bit-1chan\\\\16.wav', './piano/train/16bit-1chan\\\\17.wav', './piano/train/16bit-1chan\\\\18.wav']\n"
     ]
    }
   ],
   "source": [
    "print(fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "from functools import reduce\n",
    "import os\n",
    "import time\n",
    "from six.moves import xrange\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 audio files in specified directory\n",
      "WARNING:tensorflow:From <ipython-input-3-93deac89bd53>:58: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-8-ea00d898fb18>:16: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-1e92107c7120>:7: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d_transpose instead.\n",
      "WARNING:tensorflow:From <ipython-input-10-e8462d9e36b9>:29: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv1d instead.\n",
      "--------------------------------------------------------------------------------\n",
      "Generator vars\n",
      "[100, 32768] (3276800): G/z_project/dense/kernel:0\n",
      "[32768] (32768): G/z_project/dense/bias:0\n",
      "[1, 25, 1024, 2048] (52428800): G/upconv_0/conv2d_transpose/kernel:0\n",
      "[1024] (1024): G/upconv_0/conv2d_transpose/bias:0\n",
      "[1, 25, 512, 1024] (13107200): G/upconv_1/conv2d_transpose/kernel:0\n",
      "[512] (512): G/upconv_1/conv2d_transpose/bias:0\n",
      "[1, 25, 256, 512] (3276800): G/upconv_2/conv2d_transpose/kernel:0\n",
      "[256] (256): G/upconv_2/conv2d_transpose/bias:0\n",
      "[1, 25, 128, 256] (819200): G/upconv_3/conv2d_transpose/kernel:0\n",
      "[128] (128): G/upconv_3/conv2d_transpose/bias:0\n",
      "[1, 25, 64, 128] (204800): G/upconv_4/conv2d_transpose/kernel:0\n",
      "[64] (64): G/upconv_4/conv2d_transpose/bias:0\n",
      "[1, 25, 1, 64] (1600): G/upconv_5/conv2d_transpose/kernel:0\n",
      "[1] (1): G/upconv_5/conv2d_transpose/bias:0\n",
      "[512, 1, 1] (512): G/pp_filt/conv1d/kernel:0\n",
      "Total params: 73150465 (279.05 MB)\n",
      "--------------------------------------------------------------------------------\n",
      "Discriminator vars\n",
      "[25, 1, 64] (1600): D/downconv_0/conv1d/kernel:0\n",
      "[64] (64): D/downconv_0/conv1d/bias:0\n",
      "[25, 64, 128] (204800): D/downconv_1/conv1d/kernel:0\n",
      "[128] (128): D/downconv_1/conv1d/bias:0\n",
      "[25, 128, 256] (819200): D/downconv_2/conv1d/kernel:0\n",
      "[256] (256): D/downconv_2/conv1d/bias:0\n",
      "[25, 256, 512] (3276800): D/downconv_3/conv1d/kernel:0\n",
      "[512] (512): D/downconv_3/conv1d/bias:0\n",
      "[25, 512, 1024] (13107200): D/downconv_4/conv1d/kernel:0\n",
      "[1024] (1024): D/downconv_4/conv1d/bias:0\n",
      "[25, 1024, 2048] (52428800): D/downconv_5/conv1d/kernel:0\n",
      "[2048] (2048): D/downconv_5/conv1d/bias:0\n",
      "[32768, 1] (32768): D/output/dense/kernel:0\n",
      "[1] (1): D/output/dense/bias:0\n",
      "Total params: 69875201 (266.55 MB)\n",
      "--------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./train\\model.ckpt-1109\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1109 into ./train\\model.ckpt.\n",
      "--------------------------------------------------------------------------------\n",
      "Training has started. Please use 'tensorboard --logdir=./train' to monitor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\scipy\\io\\wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1126 into ./train\\model.ckpt.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Saving checkpoints for 1157 into ./train\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1188 into ./train\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.0937732\n",
      "INFO:tensorflow:Saving checkpoints for 1221 into ./train\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1252 into ./train\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1283 into ./train\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.10452\n"
     ]
    }
   ],
   "source": [
    "if len(fps) == 0:\n",
    "    raise Exception('Did not find any audio files in specified directory')\n",
    "print('Found {} audio files in specified directory'.format(len(fps)))\n",
    "#infer(args)\n",
    "train(fps, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "<sup>[1]</sup> [waveGAN github repository (Donahue et. al., 2019)](https://github.com/chrisdonahue/wavegan)  \n",
    "<sup>[2]</sup> [DCGAN (Radford et. al., 2016)](https://arxiv.org/pdf/1511.06434.pdf)  \n",
    "<sup>[3]</sup> [Bach piano performances](http://deepyeti.ucsd.edu/cdonahue/wavegan/data/mancini_piano.tar.gz)  \n",
    "<sup>[4]</sup> [Scipy.io.wavfile.read](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html)  \n",
    "<sup>[5]</sup> [Python assert](https://www.programiz.com/python-programming/assert-statement)  \n",
    "<sup>[6]</sup> [tf.py_func](https://www.tensorflow.org/api_docs/python/tf/py_func)  \n",
    "<sup>[7]</sup> [tf.dataset.map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)  \n",
    "<sup>[8]</sup> [tf.dataset.flat_map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map)  \n",
    "<sup>[9]</sup> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
