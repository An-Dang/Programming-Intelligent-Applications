{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further ideas\n",
    "If no embedding_matrix is assigned to the `weights`-argument of the `EmbeddingLayer` and the `trainable`-argument is set to be `True`, then the weights of the EmbeddingLayer are learned during training of the entire network. I.e. in this case no pre-trained word-embedding is required. It would be nice to compare this approach with the approach where pre-trained weights are applied. \n",
    "\n",
    "As an alternative to the CNN a LSTM-architecture, e.g. a single LSTM-Layer, followed by Dropout and a Dense Layer at the output, can be applied for classification. The input-embedding layer would be the same as for the CNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Teammember |                    |\n",
    "|------------|--------------------|\n",
    "| 1.         | Christopher Caldwell |\n",
    "| 2.         | Fabian MÃ¼ller      |\n",
    "| 3.         | An Dang         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pia4/.direnv/python-3.7.3rc1/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is loaded\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "parentDir=\"./extracted\"\n",
    "modelName=\"./DataSets/dewiki2018skipgram.model\"\n",
    "model=word2vec.Word2Vec.load(modelName)\n",
    "print(\"Model is loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:19.092680Z",
     "start_time": "2018-02-28T15:30:19.088067Z"
    }
   },
   "outputs": [],
   "source": [
    "techpath=\"./Data/GERMAN/TECH/RSS/FeedText\"\n",
    "generalpath=\"./Data/GERMAN/GENERAL/RSS/FeedText\"\n",
    "catpaths=[techpath,generalpath]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:19.366956Z",
     "start_time": "2018-02-28T15:30:19.097007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 4012 texts.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for catlabel,categorypath in enumerate(catpaths):\n",
    "    for name in sorted(os.listdir(categorypath)):\n",
    "        feedpath = os.path.join(categorypath, name)\n",
    "        if os.path.isdir(feedpath):\n",
    "            #print(name)\n",
    "            for fname in sorted(os.listdir(feedpath)):\n",
    "                fpath = os.path.join(feedpath, fname)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='utf-8')\n",
    "                t = f.read()\n",
    "                news=t.split('\\n \\n')\n",
    "                for entry in news:\n",
    "                    if (len(entry)>50) and (entry not in texts): #remove duplicates\n",
    "                        #if entry not in texts:\n",
    "                        texts.append(entry)\n",
    "                        labels.append(catlabel)\n",
    "                f.close()\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert texts to sequence of integers\n",
    "\n",
    "#### Task 10: Transform each text into a sequence of integers\n",
    "Apply the [Keras Tokenizer class](https://keras.io/preprocessing/text/#tokenizer) in order to uniquely map each word to an integer-index and to represent each text (news-item) as a sequence of integers. The maximum number of words regarded in the Tokenizer shall be `MAX_NB_WORDS=10000`. After fitting the `Tokenizer`-object with the available texts (`fit_on_texts()`), it's attribute `tokenizer.word_index` maps each word to an integer-index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:19.893962Z",
     "start_time": "2018-02-28T15:30:19.393772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS=10000\n",
    "# create tokenizer\n",
    "t = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "# fit the tokenizer on the docs\n",
    "t.fit_on_texts(texts)\n",
    "word_index = t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:20.236503Z",
     "start_time": "2018-02-28T15:30:20.180082Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# textcorpus to sequences (WordEmbedding)\n",
    "text_sequences = t.texts_to_sequences(texts)\n",
    "\n",
    "# Word-Documents into sequences\n",
    "doc_sequences =  pad_sequences(text_sequences, maxlen=35)\n",
    "\n",
    "# labels into 2d array labels for Keras\n",
    "categorical_label = to_categorical(labels, num_classes=2, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:28.636659Z",
     "start_time": "2018-02-28T15:30:28.625877Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_sequences,labels, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load  and prepare Word-Embedding\n",
    "\n",
    "#### Task 14: Load Word Embedding\n",
    "Load the Word Embedding, which has been trained and saved in task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:31:13.127470Z",
     "start_time": "2018-02-28T15:30:58.634797Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "modelName=\"./DataSets/dewiki2018skipgram.model\"\n",
    "w2v_model=word2vec.Word2Vec.load(modelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 15: Prepare `embeddings_index`\n",
    "Next a Python dictionary `embeddings_index`, which maps words to their vector-representation must be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:42:20.512927Z",
     "start_time": "2018-02-28T15:42:20.252855Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pia4/.direnv/python-3.7.3rc1/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "embedding_index = embedding_index = dict(zip(w2v_model.wv.index2word, w2v_model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_matrix = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:42:28.130765Z",
     "start_time": "2018-02-28T15:42:27.256479Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Activation, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_index)+1\n",
    "EMBEDDING_MATRIX = embedding_matrix\n",
    "MAX_SEQUENCE_LENGTH = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 35, 200)           4676800   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,797,301\n",
      "Trainable params: 4,797,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words, EMBEDDING_DIM, weights=[EMBEDDING_MATRIX], input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:49:12.003106Z",
     "start_time": "2018-02-28T15:49:11.940070Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:55:16.315866Z",
     "start_time": "2018-02-28T15:55:00.308659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3209/3209 [==============================] - 2s 594us/step - loss: 0.4187 - acc: 0.8214\n",
      "Epoch 2/10\n",
      "3209/3209 [==============================] - 1s 445us/step - loss: 0.1722 - acc: 0.9377\n",
      "Epoch 3/10\n",
      "3209/3209 [==============================] - 1s 447us/step - loss: 0.1091 - acc: 0.9629\n",
      "Epoch 4/10\n",
      "3209/3209 [==============================] - 1s 450us/step - loss: 0.0617 - acc: 0.9822\n",
      "Epoch 5/10\n",
      "3209/3209 [==============================] - 1s 438us/step - loss: 0.0449 - acc: 0.9847\n",
      "Epoch 6/10\n",
      "3209/3209 [==============================] - 1s 455us/step - loss: 0.0267 - acc: 0.9922\n",
      "Epoch 7/10\n",
      "3209/3209 [==============================] - 1s 452us/step - loss: 0.0168 - acc: 0.9956\n",
      "Epoch 8/10\n",
      "3209/3209 [==============================] - 1s 448us/step - loss: 0.0071 - acc: 0.9975\n",
      "Epoch 9/10\n",
      "3209/3209 [==============================] - 1s 449us/step - loss: 0.0016 - acc: 0.9997\n",
      "Epoch 10/10\n",
      "3209/3209 [==============================] - 1s 460us/step - loss: 9.9486e-05 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f26d5dfd710>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.91%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 35, 200)           4676800   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 35, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,797,301\n",
      "Trainable params: 4,797,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "3209/3209 [==============================] - 2s 658us/step - loss: 0.3957 - acc: 0.8283\n",
      "Epoch 2/10\n",
      "3209/3209 [==============================] - 1s 447us/step - loss: 0.1827 - acc: 0.9274\n",
      "Epoch 3/10\n",
      "3209/3209 [==============================] - 1s 444us/step - loss: 0.1252 - acc: 0.9564\n",
      "Epoch 4/10\n",
      "3209/3209 [==============================] - 1s 451us/step - loss: 0.0884 - acc: 0.9720\n",
      "Epoch 5/10\n",
      "3209/3209 [==============================] - 1s 452us/step - loss: 0.0607 - acc: 0.9801\n",
      "Epoch 6/10\n",
      "3209/3209 [==============================] - 1s 447us/step - loss: 0.0352 - acc: 0.9894\n",
      "Epoch 7/10\n",
      "3209/3209 [==============================] - 1s 445us/step - loss: 0.0234 - acc: 0.9916\n",
      "Epoch 8/10\n",
      "3209/3209 [==============================] - 1s 449us/step - loss: 0.0138 - acc: 0.9963\n",
      "Epoch 9/10\n",
      "3209/3209 [==============================] - 1s 439us/step - loss: 0.0043 - acc: 0.9984\n",
      "Epoch 10/10\n",
      "3209/3209 [==============================] - 1s 450us/step - loss: 0.0024 - acc: 0.9994\n",
      "Accuracy: 91.66%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words, EMBEDDING_DIM, weights=[EMBEDDING_MATRIX], input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As you can see, if we implement the LSTM network with a dropout layer after the LSTM the training is very good, and the accuracy after the evaluation is at 90.91%. But with two dropout layer bevor and after the LSTM the training is not that good as the model bevore but the accuracy after evaluation is better. In my opinion it is better because the training is less overfitted. \n",
    "\n",
    "\n",
    "Source:\n",
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3rc1"
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "899px",
   "left": "0px",
   "right": "1789.23px",
   "top": "153.883px",
   "width": "357px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
